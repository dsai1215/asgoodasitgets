\documentclass[twocolumn]{IEEEtran}
% \documentclass{colt2013} % Include author names

\usepackage{macros}
\pdfminorversion=4

\tikzstyle{block} = [draw, fill=white, rectangle,
    minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]
\tikzset{roads/.style={line width=0.2cm}}

\usepackage{times}

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e
\begin{document}

\title{\LARGE \bf
Stochastic Newton methods with enhanced Hessian estimation
}



\author{D. Sai Koti Reddy$^\dagger$, Prashanth L.A.$^\sharp$, Shalabh Bhatnagar$^\ddag$
\thanks{
$^\dagger$ Department of Computer Science and Automation,
Indian Institute of Science, Bangalore,
E-Mail: danda.reddy@csa.iisc.ernet.in}
\thanks{
$^\sharp$ Institute for Systems Research, University of Maryland, College Park, Maryland,
E-Mail: prashla@isr.umd.edu.
}
\thanks{
$^\ddag$ Department of Computer Science and Automation,
Indian Institute of Science, Bangalore,
E-Mail: shalabh@csa.iisc.ernet.in
}
}

\maketitle

\begin{abstract}
We propose enhancements to the Hessian estimation scheme used in two recently proposed stochastic Newton methods, based on the ideas of random directions stochastic approximation (N-RDSA-3) \cite{prashanth2015rdsa} and simultaneous perturbation stochastic approximation (N-SPSA-3) \cite{bhatnagar2015simultaneous}, respectively\footnote{The 3 in the abberviation of the algorithms is used to indicate that the algorithms in \cite{prashanth2015rdsa,bhatnagar2015simultaneous} require 3 function evaluations per iteration.}. The proposed scheme, inspired by \cite{spall-jacobian}, reduces the error in the Hessian estimate by 
\begin{inparaenum}[\bfseries (i)]
	\item incorporating a zero-mean feedback term; and
	\item optimizing the step-sizes used in the Hessian recursion.
\end{inparaenum}
We prove that N-RDSA3 and N-SPSA-3 with our Hessian improvement scheme converges asymptotically to the true Hessian.
 %Moreover, for the special of a quadratic objective, we provide a convergence rate result for 2RDSA-IH that is of the same order - in the number of iterations of Hessian recursion - as that of 2SPSA with improved Hessian estimation (2SPSA-IH) \cite{spall-jacobian}. 
The advantage with N-RDSA-3 and N-SPSA-3  is that it requires only 75\% of the simulation cost per-iteration for N-SPSA-4 with improved Hessian estimation (N-SPSA-4-IH) \cite{spall-jacobian}.
Numerical experiments show that N-RDSA-3-IH outperforms both N-SPSA-4-IH and N-RDSA-3 without the improved Hessian estimation scheme.
\end{abstract}

\begin{IEEEkeywords}
Stochastic optimization, stochastic approximation, random directions stochastic approximation (RDSA), simultaneous perturbation stochastic approximation (SPSA).
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
We consider the following \textit{simulation optimization} problem:
\begin{align}
\mbox{Find } x^* = \arg\min_{x \in \R^N} f(x). \label{eq:pb}
\end{align}
The aim is to find an algorithm that solves \eqref{eq:pb} given only noise-corrupted measurements of the objective $f$, as illustrated in Figure \ref{fig:so}.
A natural solution approach is to devise an algorithm that incrementally updates the parameter, say $x_n$, in the descent direction using the gradient and/or Hessian of the objective $f$. However, gradient/Hessian of $f$ are unavailable directly and need to be estimated from samples of $f$. 
To overcome this, we adopt the simultaneous perturbation (SP) approach - a popular and efficient idea especially in high dimensional problems - see \cite{bhatnagar-book} for a comprehensive treatment of this subject matter. 


Simultaneous perturbation stochastic approximation (SPSA) is a popular SP method. The first-order SPSA algorithm, henceforth referred to as 1SPSA, was proposed in \cite{spall}.  A closely related algorithm is random directions stochastic approximation (RDSA) \cite[pp.~58-60]{kushcla}. The gradient estimate in RDSA differs from that in SPSA, both in the construction as well as in the choice of random perturbations.  In \cite{kushcla}, the random perturbations for 1RDSA were generated by picking uniformly on the surface of a sphere and the resulting 1RDSA scheme was found to be  inferior to 1SPSA from an asymptotic convergence rate viewpoint - see \cite{chin1997comparative}.  Recent work in \cite{prashanth2015rdsa} attempts to bridge the gap between 1RDSA and 1SPSA by incorporating random perturbations based on an asymmetric Bernoulli distribution. However,  1SPSA was found to be still marginally better than 1RDSA. 
On the other hand, results in \cite{prashanth2015rdsa} show that a second order RDSA approach (referred to as N-RDSA-3 hereafter) can considerably outperform the corresponding higher order SPSA algorithm \cite{spall_adaptive} (referred to as N-SPSA-4 hereafter).  



Our work in this paper is centered on improving the N-RDSA-3 scheme of \cite{prashanth2015rdsa} by \\
\begin{inparaenum}[\bfseries (i)]
\item reducing the error in the Hessian estimate through a feedback term; and\\
\item optimizing the step-sizes used in the Hessian estimation recursion, again with the objective of improving the quality of the Hessian estimate.
\end{inparaenum}

\tikzstyle{block} = [draw, fill=white, rectangle,
   minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

 \begin{figure}[t]
    \centering
\scalebox{0.8}{\begin{tikzpicture}[auto, node distance=2cm,>=latex']
% We start by placing the blocks
\node (theta) {$\boldsymbol{x_n}$};
\node [block, fill=blue!20,right=0.6cm of theta,align=center] (sample) {\makecell{\textbf{Measurement}\\\textbf{ Oracle}}}; 
\node [right=0.6cm of sample] (end) {$\boldsymbol{\mathbf{f(x_n) + \xi_n}}$};
\node [ above right= 0.6cm of end] (bias) {\textbf{Zero mean}};
\draw [->] (theta) --  (sample);
\draw [->] (sample) -- (end);
\path [darkgreen,->] (bias) edge [bend left] (end);
\end{tikzpicture}}
\caption{Simulation optimization}
\label{fig:so}
\end{figure}

\begin{table}
\caption{A taxonomy of proposed algorithms.}
\label{tab:algos}
\centering
\begin{tabular}{c|c|c}
\toprule
\textbf{Algorithm} & \multirow{2}{*}{\textbf{Function evaluations}}\\ 
&&\\
\midrule
N-RDSA-3  & \multirow{2}{*}{$x_n, x_n\pm\delta_n d_n$}  \\ 
&&\\
\midrule
N-SPSA-3 & \multirow{2}{*}{$x_n, x_n + \delta_n \Delta_n + \delta_n \widehat \Delta_n, x_n - \delta_n \Delta_n - \delta_n \widehat \Delta_n$} \\ 
&&\\
\midrule
N-SPSA-4 & \multirow{2}{*}{$x_n\pm\delta_n\Delta_n, x_n\pm\delta_n\Delta_n+ \widehat \delta_n \widehat \Delta_n$}  \\ 
&&\\
 \bottomrule
\end{tabular}
\end{table}

Items (i) and (ii) are inspired by the corresponding improvements to the Hessian estimation recursion in N-SPSA-4 - see \cite{spall-jacobian}. While item (ii) is a relatively straightforward migration to the N-RDSA-3 setting, item (i) is a non-trivial contribution, primarily because the Hessian estimate in N-RDSA-3 is entirely different from that in N-SPSA-4 and the feedback term that we incorporate in N-RDSA-3 to improve the Hessian estimate neither correlates with that in N-SPSA-4 nor follows from the analysis in \cite{spall-jacobian}. 
The advantage with N-RDSA-3-IH is that it requires only 75\% of the simulation cost per-iteration for N-SPSA-4-IH. 

We establish that the proposed improvements to Hessian estimation in N-RDSA-3 are such  that the resulting N-RDSA-3-IH algorithm is provably convergent to the true Hessian. 
%Moreover, we show, for the special case of a quadratic objective, that	2RDSA-IH results in a convergence rate that is on par with the corresponding rate for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. 
Further, we show empirically that N-RDSA-3-IH that we propose here outperforms both N-SPSA-4-IH of \cite{spall-jacobian} and regular N-RDSA-3 of \cite{prashanth2015rdsa}. Our contribution is important because N-RDSA-3-IH, like N-RDSA-3, has lower simulation cost per iteration than N-SPSA-4 and unlike N-RDSA-3, has an improved Hessian estimation scheme.

%The rest of the paper is organized as follows: In Section~\ref{sec:2rdsa-ih}, we
%describe the improved Hessian estimation scheme, which is incorporated into the  second-order RDSA algorithm from \cite{prashanth2015rdsa}. In Section \ref{sec:2rdsa-results}, we present the theoretical results for the second-order RDSA algorithm with our improved Hessian estimation scheme.
%In Section~\ref{sec:expts}, we present the results from numerical experiments and finally, in  Section~\ref{sec:conclusions}, provide the concluding remarks.


\section{Second-order rdsa with improved hessian estimation (n-rdsa-3-ih)}
\label{sec:2rdsa-ih}
The second-order RDSA with improved Hessian estimate performs an update iteration as follows:
\begin{align}
\label{eq:e2rdsa}
x_{n+1} = & \; x_n - a_n \Upsilon(\overline H_n)^{-1}\widehat\nabla f(x_n), \\
\overline H_n = & \; (1-b_{n})  \overline H_{n-1} + b_{n} ( \widehat H_n - \widehat \Psi_n),\label{eq:2rdsa-H}
\end{align}
where $\widehat\nabla f(x_n)$ is the estimate of $\nabla f(x_n)$, 
 %and this corresponds to  the uniform  and   asymmetric Bernoulli variant gradient estimates proposed in \cite{prashanth2015rdsa}.
$\overline H_n$ is an estimate of the true Hessian ${\nabla}^2 f(\cdot)$, $\Upsilon(\cdot)$ projects any matrix onto the set of positive definite matrices and $\{a_n, n\ge 0\}$ is a step-size sequence that satisfies standard stochastic approximation conditions. There are standard procedures such as Cholesky factorization, see \cite{bert22}, for projecting a given square matrix to set of positive definite matrices. Moreover, in the vicinity of a local minimum, one expects the Hessian to be positive definite. In such a case, $\Upsilon$ will represent the identity operator.

The recursion \eqref{eq:e2rdsa} is identical to that in N-RDSA-3, while the Hessian estimation recursion \eqref{eq:2rdsa-H} differs as follows:\\
\begin{inparaenum}[\bfseries (i)]
\item  $\widehat \Psi_n$ is a zero-mean feedback term that reduces the error in Hessian estimate; and\\
\item $b_n$ is a general step-size that we optimize to improve the Hessian estimate.
\end{inparaenum}

On the other hand, $\widehat H_n$ is identical to that in N-RDSA-3, i.e., it estimates the true Hessian in each iteration using $3$ function evaluations. %Further, $\widehat H_0 = I$. 
For the sake of completeness, we provide below the construction for $\widehat\nabla f(x_n)$ and $\widehat H_n$ using asymmetric Bernoulli perturbations, before we present the feedback term that reduces the error in $\widehat H_n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\algblock{PEval}{EndPEval}
\algnewcommand\algorithmicPEval{\textbf{\em Function evaluation 1}}
 \algnewcommand\algorithmicendPEval{}
\algrenewtext{PEval}[1]{\algorithmicPEval\ #1}
\algrenewtext{EndPEval}{\algorithmicendPEval}

\algblock{PEvalPrime}{EndPEvalPrime}
\algnewcommand\algorithmicPEvalPrime{\textbf{\em Function evaluation 2}}
 \algnewcommand\algorithmicendPEvalPrime{}
\algrenewtext{PEvalPrime}[1]{\algorithmicPEvalPrime\ #1}
\algrenewtext{EndPEvalPrime}{\algorithmicendPEvalPrime}

\algblock{PImp}{EndPImp}
\algnewcommand\algorithmicPImp{\textbf{\em Gradient descent}}
 \algnewcommand\algorithmicendPImp{}
\algrenewtext{PImp}[1]{\algorithmicPImp\ #1}
\algrenewtext{EndPImp}{\algorithmicendPImp}

\algtext*{EndPEval}
\algtext*{EndPEvalPrime}
\algtext*{EndPImp}
%%%%%%%%%%%%%%%% alg-custom-block %%%%%%%%%%%%
\algblock{PEvalPrimeDouble}{EndPEvalPrimeDouble}
\algnewcommand\algorithmicPEvalPrimeDouble{\textbf{\em Function evaluation 3}}
 \algnewcommand\algorithmicendPEvalPrimeDouble{}
\algrenewtext{PEvalPrimeDouble}[1]{\algorithmicPEvalPrimeDouble\ #1}
\algrenewtext{EndPEvalPrimeDouble}{\algorithmicendPEvalPrimeDouble}
\algtext*{EndPEvalPrimeDouble}

\algblock{PImpNewton}{EndPImpNewton}
\algnewcommand\algorithmicPImpNewton{\textbf{\em Newton step}}
 \algnewcommand\algorithmicendPImpNewton{}
\algrenewtext{PImpNewton}[1]{\algorithmicPImpNewton\ #1}
\algrenewtext{EndPImpNewton}{\algorithmicendPImpNewton}

\algtext*{EndPImpNewton}
\begin{algorithm}[t]
\begin{algorithmic}
\State {\bf Input:} 
initial parameter $x_0 \in \R^N$, perturbation constants $\delta_n>0$, step-sizes $\{a_n, b_n\}$, operator $\Upsilon$.
\For{$n = 0,1,2,\ldots$}	
	\State Generate $\{d_n^{i}, i=1,\ldots,N\}$, independent of $\{d_m, m=0,1,\ldots,n-1\}$. 
	\State For any $i=1,\ldots,N$, $d_n^{i}$ is distributed either as an asymmetric Bernoulli (see \eqref{eq:det-proj}) or Uniform $U[-\eta,\eta]$ for some $\eta >0$ (see Remark \ref{remark:unif}). 
	\PEval
	    \State Obtain $y_n^+ = f(x_n+\delta_n d_n) + \xi_n^+$.
  \EndPEval
	    \PEvalPrime
	    \State Obtain $y_n^- = f(x_n-\delta_n d_n) + \xi_n^-$.
	    \EndPEvalPrime
	    	    \PEvalPrimeDouble
	    \State Obtain $y_n = f(x_n) + \xi_n$.
	    \EndPEvalPrimeDouble
	    \PImpNewton
		\State Update the parameter and Hessian as follows:
		\begin{align*}
		x_{n+1} = & \; x_n - a_n \Upsilon(\overline H_n)^{-1}\widehat\nabla f(x_n), \\
\overline H_n = &\; (1-b_{n})  \overline H_{n-1} + b_{n} ( \widehat H_n - \widehat \Psi_n),
\end{align*}
where $\widehat H_n$ is chosen either according to \eqref{eq:2rdsa-estimate-ber} or \eqref{eq:2rdsa-estimate-unif}.
		\EndPImpNewton
\EndFor
\State {\bf Return} $x_n.$
\end{algorithmic}
\caption{Structure of N-RDSA-3-IH algorithm.}
\label{alg:structure}
\end{algorithm}

Algorithm \ref{alg:structure} presents the pseudocode
\paragraph{\textbf{Function evaluations}}
Let $y_n$, $y_n^+$ and $y_n^-$ denote the function evaluations at $x_n$, $x_n+\delta_n d_n$ and $x_n - \delta_n d_n$ respectively, i.e., 
$y_n = f(x_n) + \xi_n$, $y_n^+ = f(x_n+\delta_n d_n) + \xi_n^+$ and 
$y_n^- = f(x_n-\delta_n d_n) + \xi_n^-$,
where the noise terms $\xi_n, \xi_n^+, \xi_n^-$ satisfy $\E\left[\left.\xi_n^+ + \xi_n^- - 2\xi_n \right| \F_n, d_n\right] = 0$ with $\F_n = \sigma(x_m,m < n)$ denoting the underlying sigma-field.

Further, $\delta_n, n\geq 0$ is a sequence of diminishing positive real numbers and $d_n = (d_n^1,\ldots,d_n^N)\tr$ is the perturbation random vector at instant $n$ with $d_n^i,i=1,\ldots,N$ given in \eqref{eq:det-proj}.  

\paragraph{\textbf{Gradient estimate}}
The RDSA estimate of the gradient $\nabla f(x_n)$ is given by
\begin{align}
\label{eq:grad-ber}
\widehat\nabla f(x_n) = \frac1{\lambda} d_n \left[ \dfrac{y_n^+ - y_n^-}{2\delta_n}\right],
\end{align}
where $\lambda = \E(d_n^i)^2 $ and the perturbations $d_n^i$, $i=1,\ldots,N$ are i.i.d. 
%Note that $E d_n^i = 0$, $E (d_n^i)^2 = 1+\epsilon$ and $E (d_n^i)^4 = \dfrac{(1+\epsilon)(1+(1+\epsilon)^3)}{(2+\epsilon)}$.

\paragraph{\textbf{Hessian estimate}}
\begin{align}
\label{eq:2rdsa-estimate-ber}
&\widehat H_n = M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right), \text{ where }\\
& M_n =
\left[
\begin{array}{ccc}
\frac{1}{\kappa}\left((d_n^1)^2\!-\lambda\right) & \cdots & \frac{1}{2 \lambda^2}d_n^1 d_n^N\\
\frac{1}{2 \lambda^2}d_n^2 d_n^1  &  \cdots & \frac{1}{2 \lambda^2}d_n^2 d_n^N\\
\cdots&\cdots&\cdots\\
\frac{1}{2 \lambda^2}d_n^N d_n^1 & \cdots &  \frac{1}{\kappa}\left((d_n^N)^2-\lambda\right) \\
\end{array}
\right],\nonumber
\end{align}
where $\lambda = \E(d_n^i)^2 $ , $\tau = E (d_n^i)^4$, and $\kappa = \left(\tau - \lambda^2\right)$ for any $i=1,\ldots,N$. 
%%%%%% Feedback
\paragraph{\textbf{Feedback term }$\widehat \Psi_n$}
The Hessian estimate $\widehat H_n$ can be simplified as follows: 
\begin{align}
\widehat H_n & = M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right) \nonumber \\
& =  M_n \left[\left(\dfrac{f(x_n+\delta_n d_n) + f(x_n-\delta_n d_n) - 2 f(x_n)}{\delta_n^2}\right) \right.\nonumber \\&\hspace{10em}+\left. \left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2}\right)\right] \nonumber \\
& = M_n \left(d_n\tr \nabla^2 f(x_n) d_n +  O(\delta_n^2) + \left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2} \right)\right).\label{eq:h0}
%& = M_n \left(d_n\tr \nabla^2 f(x_n) d_n +  O(\delta_n^2) + O(\delta_n^{-2})\right). \label{eq:h0}
\end{align}
For the first term on the RHS above, note that 
\begin{align}
&\E[M_n \left(d_n\tr \nabla^2 f(x_n) d_n\right) \mid \F_n] =   \E\left[\left. M_n \times\right.\right.\nonumber\\
&\left.\left.\left(\sum\limits_{i=1}^{N-1} (d_n^i)^2 \nabla^2_{ii} f(x_n) + 2\sum\limits_{i=1}^N\sum\limits_{j=i+1}^N d_n^i d_n^j \nabla^2_{ij} f(x_n) \right)\right| \F_n\right]. \label{eq:h1}
\end{align}
In analyzing the $l$th diagonal term of the above expression, the following zero-mean term appears (see the proof of Lemma 4 in \cite{prashanth2015rdsa}):
\begin{align}
& \E\left[\left. \left([M_n]_{D}\right)_{l,l} \left(2\sum\limits_{i=1}^{N-1}\sum\limits_{j=i+1}^N d_n^i d_n^j \nabla^2_{ij} f(x_n)\right)\right| \F_{n}\right] = 0,\label{eq:diagzero}
\end{align}
where for any matrix $M$, $[M]_{D}$ refers to a matrix that retains only the diagonal entries of $M$ and replaces all the remaining  entries with zero, and $\left([M]_{D}\right)_{i,j}$ refers to the $(i,j)$th entry in $[M]_D$. We shall also use $[M]_{N}$ to refer to a matrix that retains only the off-diagonal entries of $M$, while replaces all the diagonal entries with zero.

The term on the LHS in \eqref{eq:diagzero}, denoted by $\Psi_{n}^{1}(\nabla^2 f(x_n))$, can be written in matrix form as follows: 
\begin{align}\label{eq:digmat}
\Psi_{n}^{1}(\nabla^2 f(x_n)) = [M_n]_{D}\left(d_{n}\tr \, [\nabla^2 f(x_n)]_{N} \, d_{n}\right).
\end{align}

In analyzing the off-diagonal term ($(k,l)$ where $k < l$) of \eqref{eq:h1}, the following zero-mean term appears:
\begin{align}
& \E\left[\left.\left([M_n]_{D}\right)_{k,l}   \left(\sum\limits_{i=1}^N (d_n^i)^2 \nabla^2_{ii} f(x_n)\right)\right| \F_n \right] = 0. 
\end{align}
The term on the LHS above, denoted by $\Psi_{n}^{2}(\nabla^2 f(x_n))$, can be written in matrix form as follows: 
\begin{align}\label{eq:ndmat}
\Psi_{n}^{2}(\nabla^2 f(x_n)) = [M_n]_{N}\left(d_{n}\tr \, [\nabla^2 f(x_n)]_{D} \, d_{n}\right).
\end{align}
From the foregoing, the per-iteration Hessian estimate $\widehat H_n$ can be re-written as follows:
\begin{align}
 \E\left[\left.\widehat H_n\right| \F_n\right] = & \nabla^2 f(x_n) + \E\left[\left.\Psi_{n}(\nabla^2 f(x_n))\right| \F_n\right]  +  O(\delta_n^2)\nonumber\\
&+\E\left[\left.\left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2} \right)\right| \F_n\right], \label{eq:hnhat}
\end{align}
where, for any matrix $H$, 
\begin{align}
&\Psi_{n}(H) = \Psi_{n}^{1}(H) + \Psi_{n}^{2}(H)\nonumber\\
&\qquad= [M_n]_{D}\left(d_{n}\tr \, [H]_{N} \, d_{n}\right) +  [M_n]_{N}\left(d_{n}\tr \, [H]_{D} \, d_{n}\right).\label{eq:psi}
\end{align}
In the RHS of \eqref{eq:hnhat}, it is easy to see that  the second term involving $\Psi_{n}$ and the last term involving the noise are zero-mean. Moreover, since the noise is bounded by assumption, the last term in \eqref{eq:hnhat} vanishes asymptotically at the rate $O(\delta_n^{-2})$. So, the error in estimating the Hessian is due to the second term, which involves the perturbations $d_n$. This motivates the term $\widehat \Psi_n$ in the update rule \eqref{eq:e2rdsa}. 
 %and using the fact that $\E\left[\left.\xi_n^+ + \xi_n^- - 2\xi_n \right| \F_n\right] = 0$


Given that we operate in a simulation optimization setting, which implies $\nabla^2 f$ is not known, we construct the feedback term $\widehat \Psi_n$ in \eqref{eq:e2rdsa} by using $\overline H_{n-1}$ as a proxy for $\nabla^2 f$, i.e.,
\begin{align}
\widehat \Psi_n = \Psi_{n} (\overline H_{n-1}).
\end{align}

\paragraph{\textbf{Optimizing the step-sizes }$b_n$}
Unlike the feedback term, adapting the idea of optimizing the step-sizes for N-RDSA-3 is relatively straightforward from the corresponding approach for N-SPSA-4 in \cite{spall-jacobian}. The difference here is that there exists only one $N$-dimensional perturbation vector $d_n$ in our setting, while N-SPSA-4 required two such vectors. This in turn implies that only the perturbation constant $\delta_n$ is needed in optimizing $b_n$.

The optimal choice for $b_n$ in \eqref{eq:2rdsa-H} is the following:
\begin{align}
\label{eq:wieghts}
b_i  = \delta_i^{4}/\sum\limits_{j=0}^{i} \delta_j^{4}.
\end{align}
The main idea behind the above choice is provided below.
From \eqref{eq:hnhat}, we can infer that
\begin{align*}
\E \parallel \widehat H_n \parallel^2 \le \dfrac{C}{\delta_n^4} \text{ for some } C<\infty. 
\end{align*} 
This is because the third term in \eqref{eq:hnhat} vanishes asymptotically, while the fourth term there dominates asymptotically. Moreover, the noise factors in the fourth term in \eqref{eq:hnhat} are bounded above due to (C9) and independent of $n$, leaving the $\delta_n^2$ term in the denominator there. 

So, the optimization problem to be solved at instant $n$ is as follows:
\begin{align}
\sum \limits_{i=0}^{n} (\tilde b_k)^2 \delta_i^{-4}, \text{ subject to} \label{eq:wn-opt}\\
\tilde b_i \geq 0 ~\forall i \text{ and }\sum \limits_{i=0}^{n} \tilde b_i = 1.
\end{align}
The optimization variable $\tilde b_i$ from the above is related to the Hessian recursion \eqref{eq:2rdsa-H} as follows:
\begin{align}
\label{eq:hess}
\overline H_n = \sum\limits_{i=0}^{n} \tilde b_k(\widehat H_i -\widehat \Psi_i).
\end{align}
The solution to \eqref{eq:wn-opt} is achieved for $\tilde b_i^* = \delta_i^{4}/\sum \limits_{j=0}^{n} \delta_j^{4}, i=1,\ldots,n$. The optimal choice $\tilde b_i^*$ can be translated to the step-sizes $b_i$, leading to \eqref{eq:wieghts}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In \cite{prashanth2015rdsa}, the authors suggest two alternatives for the distribution of random perturbations $d_n$: the asymmetric Bernoulli and uniform.
\begin{remark}\textbf{\textit{(Asymmetric Bernoulli)}}
\label{remark:asy-ber}
\paragraph{\textbf{Gradient estimate}}
The RDSA estimate of the gradient $\nabla f(x_n)$ is given by
\begin{align}
\label{eq:grad-ber}
\widehat\nabla f(x_n) = \frac1{\lambda} d_n \left[ \dfrac{y_n^+ - y_n^-}{2\delta_n}\right],
\end{align}
where $\lambda = \E(d_n^i)^2 = (1+\epsilon)$ and the perturbations $d_n^i$, $i=1,\ldots,N$ are i.i.d. and distributed  as follows: 
\begin{equation}
\label{eq:det-proj}
 d_n^i =
  \begin{cases}
   -1 &  \text{ w.p. } \dfrac{1+\epsilon}{(2+\epsilon)}, \\
   1+\epsilon &  \text{ w.p. } \dfrac{1}{(2+\epsilon)},
  \end{cases}
\end{equation}
with $\epsilon>0$ being a constant that can be chosen to be arbitrarily small.
%Note that $E d_n^i = 0$, $E (d_n^i)^2 = 1+\epsilon$ and $E (d_n^i)^4 = \dfrac{(1+\epsilon)(1+(1+\epsilon)^3)}{(2+\epsilon)}$.
\paragraph{\textbf{Hessian estimate}}

\begin{align}
\label{eq:2rdsa-estimate-ber}
&\widehat H_n = M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right), \text{ where }\\
& M_n =
\left[
\begin{array}{ccc}
\frac{1}{\kappa}\left((d_n^1)^2\!-\lambda\right) & \cdots & \frac{1}{2 \lambda^2}d_n^1 d_n^N\\
\frac{1}{2 \lambda^2}d_n^2 d_n^1  &  \cdots & \frac{1}{2 \lambda^2}d_n^2 d_n^N\\
\cdots&\cdots&\cdots\\
\frac{1}{2 \lambda^2}d_n^N d_n^1 & \cdots &  \frac{1}{\kappa}\left((d_n^N)^2-\lambda\right) \\
\end{array}
\right],\nonumber
\end{align}
where $\lambda = \E(d_n^i)^2 = (1+\epsilon)$ , $\tau = E (d_n^i)^4= \dfrac{(1+\epsilon)(1+(1+\epsilon)^3)}{(2+\epsilon)}$, and $\kappa = \left(\tau - \lambda^2\right)$ for any $i=1,\ldots,N$. 
\end{remark}

\begin{remark}\textbf{\textit{(Uniform perturbations)}}
\label{remark:unif}
Choose $d_n^i$, $ i=1,\ldots,N$ to be i.i.d. $U[-\eta,\eta]$ for some $\eta>0$, where $U[-\eta,\eta]$ denotes the uniform distribution on the interval $[-\eta,\eta]$.
Then, the RDSA estimate of the gradient is given by
\begin{align}
\label{eq:grad-unif}
\widehat\nabla f(x_n) = \frac1{\lambda} d_n \left[ \dfrac{y_n^+ - y_n^-}{2\delta_n}\right].
\end{align}

The Hessian estimate in this case is given by
\begin{align}
\label{eq:2rdsa-estimate-unif}
&\widehat H_n =  M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right), \text{ where } \\
& M_n =
\left[
\begin{array}{cccc}
\frac{1}{\kappa}\left((d_n^1)^2-\lambda\right) & \cdots &\frac{1}{2 \lambda^2} d_n^1 d_n^N\\
\frac{1}{2 \lambda^2} d_n^2 d_n^1  &  \cdots & \frac{1}{2 \lambda^2} d_n^2 d_n^N\\
\cdots&\cdots&\cdots\\
\frac{1}{2 \lambda^2} d_n^N d_n^1 & \cdots &  \frac{1}{\kappa}\left((d_n^N)^2-\lambda\right) \\
\end{array}
\right].\nonumber
\end{align}
where $\lambda = \E(d_n^i)^2 = \frac{\eta^2}{3}$, $\tau = E (d_n^i)^4= \frac{\eta^4}{5}$ and $\kappa = \left(\tau - \lambda^2\right)$ for any $i=1,\ldots,N$. 


The feedback term that we proposed is applicable to  both asymmetric Bernoulli case and uniform perturbations.
\end{remark}


\section{Second-order spsa with improved hessian estimation (n-spsa-3-ih)}

\paragraph{\textbf{Function evaluations}}
Let $y_n$, $y_n^+$ and $y_n^-$ denote the function evaluations at $x_n$, $x_n+\delta_n \Delta_n+\delta_n \widehat \Delta_n$ and $x_n -\delta_n\Delta_n-\delta_n\widehat\Delta_n$ respectively, i.e., 
$y_n = f(x_n) + \xi_n$, $y_n^+ = f(x_n+\delta_n \Delta_n +\delta_n \widehat\Delta_n) + \xi_n^+$ and 
$y_n^- = f(x_n -\delta_n\Delta_n-\delta_n\widehat\Delta_n) + \xi_n^-$,
where the noise terms $\xi_n, \xi_n^+, \xi_n^-$ satisfy $\E\left[\left.\xi_n^+ + \xi_n^- - 2\xi_n \right| \F_n, \Delta_n, \widehat \Delta_n\right] = 0$ with $\F_n = \sigma(x_m,m <  n)$ denoting the underlying sigma-field.

Further, $\delta_n, n\geq 0$ is a sequence of diminishing positive real numbers and $\Delta_n = (\Delta_n^1,\ldots,\Delta_n^N)\tr$ , $\widehat\Delta_n = (\widehat\Delta_n^1,\ldots,\widehat\Delta_n^N)\tr$ are the perturbation random vector at instant $n$ with $\Delta_n^i,i=1,\ldots,N$, and $\widehat\Delta_n^i,i=1,\ldots,N$ are independent identically distributed (i.i.d), mean-zero random varibles having finite inverse movements of order greater than 2 and satisfy the conditions (C16), C(21).  

\paragraph{\textbf{Gradient estimate}}
The SPSA estimate of the gradient $\nabla f(x_n)$ is given by
\begin{align}
\label{eq:grad-spsa}
\widehat\nabla_{(i)} f(x_n) =  \left[ \dfrac{y_n^+ - y_n^-}{2\delta_n \Delta_n^{(i)}}\right],
\end{align}
where the perturbations $\Delta_n^i$, $i=1,\ldots,N$ are i.i.d. and as mentioned above.

%Note that $E d_n^i = 0$, $E (d_n^i)^2 = 1+\epsilon$ and $E (d_n^i)^4 = \dfrac{(1+\epsilon)(1+(1+\epsilon)^3)}{(2+\epsilon)}$.

\paragraph{\textbf{Hessian estimate}}
The $i,j$th entry of the Hessian estimate in this case is given by
\begin{align}
\label{eq:2spsa-ber}
&\left(\widehat H_n\right)_{ij} =  \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{2\delta_n^2 \Delta_n^{(i)} \widehat \Delta_n^{(j)}}\right), 
\end{align}
%%%%%% Feedback
\paragraph{\textbf{Feedback term }$\widehat \Psi_n$}
The $i,j$th term of the Hessian estimate $\widehat H_n$ can be simplified as follows: 
\begin{align}
\left(\widehat H_n \right)_{ij} & =  \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{2\delta_n^2 \Delta_n^{(i)} \widehat \Delta_n^{(j)}}\right) \nonumber \\
%& =  \left[\left(\dfrac{f(x_n+\delta_n \Delta_n+\delta_n \widehat \Delta_n) + f(x_n -\delta_n\Delta_n-\delta_n\widehat\Delta_n) - 2 f(x_n)}{\delta_n^2 \Delta_n^{(i)} \widehat \Delta_n^{(j)}}\right) \right.\nonumber \\&\hspace{10em}+\left. \left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2}\right)\right] \nonumber \\
& = \left( \frac{(\Delta_n+ \widehat \Delta_n)\tr \nabla^2 f(x_n) (\Delta_n+ \widehat \Delta_n)}{2\Delta_n^{(i)} \widehat \Delta_n^{(j)}} +  O(\delta_n^2) + \right.\nonumber \\&\hspace{10em}+\left.\left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{2\delta_n^2 \Delta_n^{(i)} \widehat \Delta_n^{(j)}} \right)\right).\label{eq:h3}
%& = M_n \left(d_n\tr \nabla^2 f(x_n) d_n +  O(\delta_n^2) + O(\delta_n^{-2})\right). \label{eq:h0}
\end{align}
After expanding the first term on the RHS above, we get
\begin{align}
\sum\limits_{l=1}^N\sum\limits_{m=1}^N \frac{\Delta_n^l \nabla^2_{lm} f(x_n) \Delta_n^m}{2\Delta_n^i \widehat\Delta_n^j} &+  \sum\limits_{l=1}^N\sum\limits_{m=1}^N \frac{\Delta_n^l \nabla^2_{lm} f(x_n) \widehat\Delta_n^m}{\Delta_n^i \widehat\Delta_n^j} \nonumber \\ & + \sum\limits_{l=1}^N\sum\limits_{m=1}^N \frac{\widehat\Delta_n^l \nabla^2_{lm} f(x_n)\widehat \Delta_n^m}{2\Delta_n^i \widehat\Delta_n^j}\label{eq:2spsaexp}
\end{align}
It is easy to see that 
\begin{align}
&\E\left[\left.\sum\limits_{l=1}^N\sum\limits_{m=1}^N \frac{\Delta_n^l \nabla^2_{lm} f(x_n) \Delta_n^m}{\Delta_n^i \widehat\Delta_n^j} \right | \F_n\right] = 0 \nonumber\\ &\E\left[\left.\sum\limits_{l=1}^N\sum\limits_{m=1}^N \frac{\widehat\Delta_n^l \nabla^2_{lm} f(x_n) \widehat\Delta_n^m}{\Delta_n^i \widehat\Delta_n^j }\right | \F_n\right] = 0 \nonumber\\ &\E\left[\left. \sum\limits_{l=1}^N\sum\limits_{m=1}^N \frac{\Delta_n^l \nabla^2_{lm} f(x_n) \widehat\Delta_n^m}{\Delta_n^i \widehat\Delta_n^j} \right | \F_n\right] = \nabla^2_{ij} f(x_n) \hspace{0.3cm}a.s..
\end{align}
From the above equation we know that first and second terms are mean zero terms, they can be represented in matrix form as below
\begin{align}
\Psi_n^1(\nabla^2 f(x_n)) =\frac{1}{2} M_n \left[\Delta_n\tr \nabla^2 f(x_n) \Delta_n + \widehat\Delta_n\tr \nabla^2 f(x_n) \widehat\Delta_n\right]
\end{align}
where $M_n = [1/\Delta_n^1, \ldots ,1/\Delta_n^N]\tr [1/\Delta_n^1, \ldots ,1/\Delta_n^N]$.
Now consider the second term in the \eqref{eq:2spsaexp}. It can be written as following 
\begin{align}
\nabla^2_{ij} f(x_n) + \frac{1}{\Delta_n^i \widehat\Delta_n^j} \mathop{\sum_{l=1}^N \sum_{m=1}^{N}}_{lm \neq ij} \Delta_n^l \nabla^2_{lm} f(x_n) \widehat\Delta_n^m
\end{align}
Now in the above equation the second term is mean zero term and it can be represented as follows
\begin{align}
\Psi_n^2(\nabla^2 f(x_n)) = \widehat N_n\tr \nabla^2 f(x_n) N_n  &+ \widehat N_n\tr \nabla^2 f(x_n) \nonumber\\ &+ \nabla^2 f(x_n) N_n
\end{align}
where $N_n,\widehat N_n$ are defined as follows $N_n = \Delta_n \left[\frac{1}{\Delta_n^1},\ldots,\frac{1}{\Delta_n^N}\right] - I_N$ and $\widehat N_n = \widehat \Delta_n \left [\frac{1}{\widehat\Delta_n^1},\ldots,\frac{1}{\widehat \Delta_n^N}\right] - I_N$. Where $I_N$ is $N \times N$ identity matrix.
From the foregoing, the per-iteration Hessian estimate $\widehat H_n$ can be re-written as follows:
\begin{align}
H_n = & \nabla^2 f(x_n) + \Psi_{n}(\nabla^2 f(x_n)) +  O(\delta_n^2) + O(\delta_n^{-2})  \label{eq:spsahnhat}
\end{align}
where, for any matrix $H$, 
\begin{align}
\Psi_{n}(H) &=  \Psi_{n}^{1}(H) + \Psi_{n}^{2}(H)\nonumber\\
&=  \frac{1}{2}M_n \left[\Delta_n\tr H \Delta_n + \widehat\Delta_n\tr H \widehat\Delta_n\right] + \widehat N_n\tr H N_n  \nonumber\\ & \hspace{3.5cm}+ \widehat N_n\tr H + H N_n.\label{eq:spsapsi}
\end{align}



Given that we operate in a simulation optimization setting, which implies $\nabla^2 f$ is not known, we construct the feedback term $\widehat \Psi_n$ in \eqref{eq:e2rdsa} by using $\overline H_{n-1}$ as a proxy for $\nabla^2 f$, i.e.,
\begin{align}
\widehat \Psi_n = \Psi_{n} (\overline H_{n-1}).
\end{align}

\paragraph{\textbf{Optimizing the step-sizes }$b_n$}

The optimal choice for $b_n$ in \eqref{eq:2rdsa-H}  is the following:
\begin{align}
\label{eq:spsawieghts}
b_i  = \delta_i^{4}/\sum\limits_{j=0}^{i} \delta_j^{4}.
\end{align}
The main idea behind the above choice is provided below.
From \eqref{eq:spsahnhat}, we can infer that
\begin{align*}
\E \parallel \widehat H_n \parallel^2 \le \dfrac{C}{\delta_n^4} \text{ for some } C<\infty. 
\end{align*} 
This is because the third term in \eqref{eq:spsahnhat} vanishes asymptotically, while the fourth term there dominates asymptotically. Moreover, the noise factors in the fourth term in \eqref{eq:spsahnhat} are bounded above due to (C21)  and independent of $n$, leaving the $\delta_n^2$ term in the denominator there. 

So, the optimization problem to be solved at instant $n$ is as follows:
\begin{align}
\sum \limits_{i=0}^{n} (\tilde b_k)^2 \delta_i^{-4}, \text{ subject to} \label{eq:spsawn-opt}\\
\tilde b_i \geq 0 ~\forall i \text{ and }\sum \limits_{i=0}^{n} \tilde b_i = 1.
\end{align}
The optimization variable $\tilde b_i$ from the above is related to the Hessian recursion \eqref{eq:2rdsa-H} as follows:
\begin{align}
\label{eq:spsahess}
\overline H_n = \sum\limits_{i=0}^{n} \tilde b_k(\widehat H_i -\widehat \Psi_i).
\end{align}
The solution to \eqref{eq:spsawn-opt} is achieved for $\tilde b_i^* = \delta_i^{4}/\sum \limits_{j=0}^{n} \delta_j^{4}, i=1,\ldots,n$. The optimal choice $\tilde b_i^*$ can be translated to the step-sizes $b_i$, leading to \eqref{eq:spsawieghts}.







\section{Convergence analysis for n-rdsa-3-ih}
\label{sec:2rdsa-results}
\input{results}

\section{Convergence analysis for n-spsa-3-ih}
\label{sec:2spsa-results}
We make the same assumptions as those used in the analysis of \cite{prashanth2015rdsa}, with a few minor alterations. The assumptions are listed below:
\begin{enumerate}[label=(\textbf{C\arabic*}),resume]
\item  The function
$f$ is four-times differentiable\footnote{Here $\nabla^4 f(x) = \dfrac{\partial^4 f (x)}{\partial x\tr \partial x\tr \partial x\tr \partial x\tr}$ denotes the fourth derivate of $f$ at $x$ and $\nabla^4_{i_1 i_2 i_3 i_4} f(x)$ denotes the $(i_1 i_2 i_3 i_4)$th entry of $\nabla^4 f(x)$, for $i_1, i_2, i_3,i_4=1,\ldots, N$.} with $\left|\nabla^4_{i_1 i_2 i_3 i_4} f(x) \right| < \infty$, for $i_1, i_2, i_3,i_4=1,\ldots, N$ and for all $x\in \R^N$. 

%\item  For some $\rho>0$  and almost all $x_n$, the function $f$ is four-times differentiable with a uniformly (in $n$) bounded fourth derivative for all $x$ such that $\left\| x_n - x\right\| \le \rho$. 

\item For each $n$ and all $x$, there exists a $\rho>0$ not dependent on $n$ and $x$, such that $(x-x^*)\tr \bar f_n(x) \ge \rho \left\| x_n - x\right\|$, where $\bar f_n(x) = \Upsilon(\overline H_n)^{-1} \nabla f(x)$.

\item $\{\xi_n, \xi_n^+,\xi_n^-, n=1,2,\ldots\}$ are such that, for all $n$, $\E\left[\left. \xi_n^+ + \xi_n^- - 2 \xi_n \right| \F_n, \Delta_n, \widehat \Delta_n \right] = 0$, where $\mathcal{F}_n = \sigma(x_m,m < n)$ denotes the underlying sigma-field.. 

\item $\{\Delta_n^i,\widehat\Delta_n^i, i=1,\ldots,N, n=1,2,\ldots\}$ are i.i.d independent of $\F_n$ and for some $\alpha_0 > 0$ and for all $n,l$ $|\Delta_{nl}|\le \alpha_0$, $\Delta_{nl}$ is symmetrically distributed about 0, $\Delta_{nl}$ are mutually independent across $n$ and $l$ and they satisfy $\E (\Delta_{nl}^{-2})$,$\E (\widehat\Delta_{nl}^{-2}) \le \alpha_0$.

\item  The step-sizes $a_n$ and perturbation constants $\delta_n$ are positive, for all $n$ and satisfy
$$\hspace{-1.7em} a_n, \delta_n \rightarrow 0\text{ as } n \rightarrow \infty, 
\sum_n a_n=\infty \text{ and } \sum_n \left(\frac{a_n}{\delta_n}\right)^2 <\infty.$$

\item For each $i=1,\ldots,N$ and any $\rho>0$, 
$P(\{ \bar f_{ni} (x_n) \ge 0 \text{ i.o}\} \cap \{ \bar f_{ni} (x_n) < 0 \text{ i.o}\} \mid \{ |x_{ni} - x^*_i| \ge \rho\quad \forall n\}) =0.$

\item The operator $\Upsilon$ satisfies $\delta_n^2 \Upsilon(H_n)^{-1} \rightarrow 0$ a.s. and  $E(\left\| \Upsilon(H_n)^{-1}\right\|^{2+\zeta}) \le \rho$ for some $\zeta, \rho>0$.

\item For any $\tau >0$ and nonempty $S \subseteq \{1,\ldots,N\}$, there exists a $\rho'(\tau,S)>\tau$ such that 
$$ \limsup_{n\rightarrow \infty} \left| \dfrac{\sum_{i \notin S} (x-x^*)_i \bar f_{ni}(x)}{\sum_{i \in S} (x-x^*)_i \bar f_{ni}(x)}               \right| < 1 \text{ a.s.}$$
for all $|(x-x^*)_i| < \tau$ when $i \notin S$ and $|(x-x^*)_i| \ge \rho'(\tau,S)$ when $i\in S$.
\item For some $\alpha_0, \alpha_1, \alpha_2 >0$ and for all $n,l,m$, $\E {\xi_n}^{2} \le \alpha_0$, $\E {\xi_n^{\pm}}^{2} \le \alpha_0$, $\E f(x_n)^{2} \le \alpha_1$,  $\E f(x_n + \delta_n \Delta_n + \delta_n \widehat \Delta_n)^{2},\E f(x_n - \delta_n \Delta_n - \delta_n \widehat \Delta_n)^{2} \le \alpha_1$, 
%$\E \left[ f(x_n + \delta_n \Delta_n + \delta_n \widehat \Delta_n)^{2}/ (\Delta_{nl}\widehat\Delta_{nm})^2 \mid \F_n\right]$,
$\E \left[| f(x_n + \delta_n \Delta_n + \delta_n \widehat \Delta_n)/ (\Delta_{nl}\widehat\Delta_{nm})|^{2+\alpha_2} \mid \F_n\right]$,
%$\E \left[ f(x_n - \delta_n \Delta_n - \delta_n \widehat \Delta_n)^{2}/ (\Delta_{nl}\widehat\Delta_{nm})^2 \mid \F_n\right]$,
$\E \left[ |f(x_n - \delta_n \Delta_n - \delta_n \widehat \Delta_n)/ (\Delta_{nl}\widehat\Delta_{nm})|^{2+\alpha_2} \mid \F_n\right]$,
 $\E\left[\left. (\xi_n^+ + \xi_n^- - 2 \xi_n)^2/(\Delta_{nl}\widehat\Delta_{nm})^2 \right| \F_n \right] \le \alpha_1$ 
and $\E \left(\left\| \Upsilon(\overline H_n) \right\|^2 \mid \F_n\right) \le \alpha_1$. 
\item  $\delta_n = \frac{\delta_0}{(n+1)^{\varsigma}}$, where $\delta_0 > 0$ and $0 < \varsigma \le 1/8$.
\end{enumerate}
The reader is referred to Section II-B of \cite{prashanth2015rdsa} for a detailed discussion of the above assumptions. We remark here that (C13)-(C20) are identical to that in \cite{prashanth2015rdsa}, while (C21) and (C22) introduce minor additional requirements on $\left\| \Upsilon(\overline H_n \right\|^2$ and $\delta_n$, respectively and these are inspired by \cite{spall-jacobian}.

\begin{lemma}(\textbf{N-SPSA-3-IH Bias in Hessian estimate})
\label{lemma:2spsa-bias}
Under \textbf{(C13)-(C22) equivalent}, with $\widehat H_n$ defined according to  \eqref{eq:2spsa-ber} , we have a.s. that\footnote{Here $\widehat H_n(i,j)$ and $\nabla^2_{ij}f(\cdot)$ denote the $(i,j)$th entry in the Hessian estimate $\widehat H_n$ and the true Hessian $\nabla^2 f(\cdot)$, respectively.}, for $i,j = 1,\ldots,N$,
\begin{align}
\left|\E\left[
\left. \widehat H_n(i,j) \right| \F_n \right] - \nabla^2_{ij} f(x_n)\right| = O(\delta_n^2).
\end{align} 
\end{lemma}
\begin{proof}
See Proposition 4.2 in \cite{bhatnagar2015simultaneous}.
\end{proof}

\begin{theorem}(\textbf{N-SPSA-3-IH Strong Convergence of Hessian})
\label{thm:2spsa-H}
Under (C13)-(C22), we have that 
$$\overline H_n \rightarrow \nabla^2 f(x^*) \text{ a.s. as } n\rightarrow \infty.$$ 
In the above, $\overline H_n$ is updated according to \eqref{eq:2rdsa-H}. $\widehat H_n$ defined according to \eqref{eq:2spsa-ber} and the step-sizes $b_n$ are chosen as suggested in \eqref{eq:spsawieghts}. 
\end{theorem}
\begin{proof}
This proof is similar to the proof of the theorem \ref{thm:2rdsa-H}
\end{proof}

We next present a convergence rate result for the special case of a quadratic objective function under the following additional assumptions:
\begin{enumerate}[label=(\textbf{C\arabic*}),resume]
\item  $f$ is quadratic and $\nabla^2 f(x^*) > 0$. 
\item The operator $\Upsilon$ is chosen such that $\E(\parallel \Upsilon(\overline H_n) - \overline H_n\parallel^2) = o(e^{-2wn^{1-r}/(1-r)})$ and $\parallel \Upsilon(H) - H \parallel^2 / (1+\parallel H \parallel^2)$ is uniformly bounded.
\end{enumerate}
\begin{theorem}(\textbf{N-SPSA-3-IH Quadratic case - Convergence rate})
\label{thm:spsaquad-bound}
Assume (C16), (C22), (C23) and (C24) and also that the setting is noise-free. 
Let $b_n = b_0/n^r$, $n=1,2,\ldots,k$, where $1/2 < r < 1$ and $0 < b_0 \leq 1$. For notational simplicity, let $H^*=\nabla^2 f(x^*)$. Letting $\Lambda_k = \overline H_k - H^*$, we have 
\begin{align}
\text{trace}[\E (\Lambda_n \tr \Lambda_n)] = O(e^{-2b_0n^{1-r} / {1-r}}).
\label{eq:spsaquad-bigo}
\end{align}
\end{theorem}
\begin{proof}
This proof follows from the proof of theorem 3 of \cite{spall-jacobian}
\end{proof}





\section{Numerical Experiments}
\label{sec:expts}
\subsection{Implementation}
We test the performance of N-RDSA-3-Unif, N-RDSA-3-AsymBer and N-SPSA-4, with/without improved Hessian estimation. 
N-SPSA-4 algorithm uses Bernoulli $\pm 1$-valued perturbations, while N-RDSA-3/N-RDSA-3-IH come in two variants - one that uses $U[-1,1]$ distributed perturbations (referred to as N-RDSA-3-Unif/N-RDSA-3-IH-Unif) and the other that uses asymmetric Bernoulli perturbations (referred to as N-RDSA-3-AsymBer/N-RDSA-3-IH-AsymBer)\footnote{The implementation is available at \url{https://github.com/prashla/RDSA/archive/master.zip}.}.

For the empirical evaluations, we use the following two loss functions in $N=10$ dimensions:
\paragraph{Quadratic loss}
\begin{align}
f(x) = x\tr A x + b\tr x.\label{eq:quadratic}
\end{align} 
The optimum $x^*$ for the above $f$ is such that each coordinate of $x^*$ is $-0.9091$, with $f(x^*) = -4.55$.

\paragraph{Fourth-order loss}
\begin{align} 
f(x) = x\tr A\tr A x + 0.1 \sum_{j=1}^N (Ax)^3_j + 0.01 \sum_{j=1}^N (Ax)^4_j.\label{eq:4thorder}
 \end{align} 
The optimum $x^*$ for above $f$ is $x^*=0$, with $f(x^*) = 0$. 

In both functions, $A$ is such that $NA$ is an upper triangular matrix with each entry one, $b$ is the $N$-dimensional vector of ones and the noise structure is similar to that used in \cite{spall_adaptive}. For any $x$, the noise is $[x\tr, 1]z$, where $z \approx \N(0,\sigma^2 I_{11\times11})$. We perform experiments for noisy as well as noise-less settings, with $\sigma=0.1$ for the noisy case. 



For all algorithms, we set $\delta_n = 3.8/n^{0.101}$ and $a_n = 1/n^{0.6}$, while $b_n$ are set according to \eqref{eq:wieghts}. These choices have been used  for N-SPSA-4 implementations before (see \cite{spall_adaptive}) and have demonstrated good finite-sample performance empirically, while satisfying the theoretical requirements needed for asymptotic convergence.  For all the algorithms, the initial point $x_0$ is the $N$-dimensional vector of ones.  For both N-SPSA-4 and N-RDSA-3/N-RDSA-3-IH, an initial $20\%$ of the  simulation budget was used up by 1SPSA/1RDSA and the resulting iterate was used to initialize N-SPSA-4/N-RDSA-3. The distribution parameter $\epsilon$ is set to $0.0001$ for N-RDSA-3 and to $0.01$ for 1RDSA. 

\subsection{Results}
We use normalized loss and normalized MSE (NMSE) as performance metrics for evaluating the algorithms. 
NMSE is the ratio $\l x_{n_\text{end}} - x^* \r^2 / \l x_0 - x^*\r^2$, while normalized loss is the ratio $f(x_{n_\text{end}})/f(x_0)$.  Here $n_\text{end}$ denotes the iteration number when the algorithm stopped updating its parameter. Note that $n_\text{end}$ is a function of the simulation budget. N-RDSA-3/N-RDSA-3-IH use only three simulations per-iteration and hence, $n_\text{end}$ is $1/3$rd of the simulation budget, while it is $1/4$th of the simulation budget for N-SPSA-4, since the latter algorithm uses four simulations per-iteration. 

\begin{table}
\centering
 \caption{Normalized loss values for fourth-order  objective \eqref{eq:4thorder} with and without noise: standard error from $500$ replications shown after $\pm$}
\label{tab:norloss-4thf}
\begin{tabular}{|c|c|c|}
\toprule
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0.1$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{N-SPSA-4} & $0.132 \pm 0.0267$ & $0.104 \pm 0.0355$\\
&&\\
\textbf{N-RDSA-3-Unif} &$0.115 \pm 0.0214$ & $0.0271 \pm 0.0538$\\ 
&&\\
\textbf{N-RDSA-3-AsymBer}& $0.0471 \pm 0.021$& $\bm{0.0099 \pm 0.0014}$\\
 \bottomrule
 %%%%%%%%%%%%2nd ordr
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{N-SPSA-4} & $0.0795 \pm 0.0234$ & $0.0628 \pm 0.0234$\\
&&\\
\textbf{N-RDSA-3-Unif} &$0.0813 \pm 0.0275$ & $0.0214 \pm 0.00376$\\ 
&&\\
\textbf{N-RDSA-3-AsymBer}& $0.0199 \pm 0.0114$& $\bm{0.0098 \pm 0.00147}$\\
 \bottomrule
\end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%% Quadratic
\begin{table}
\centering
 \caption{Normalized loss values for quadratic objective \eqref{eq:quadratic} with and without noise: standard error from $500$ replications shown after $\pm$}
\label{tab:norloss-quadratic}
\begin{tabular}{|c|c|c|}
\toprule
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0.1$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{N-SPSA-4} & $-0.0062 \pm 0.1164$ & $-0.1229 \pm 0.1374$\\
&&\\
\textbf{N-RDSA-3-Unif} &$0.0485 \pm 0.1465$ & $-0.259 \pm 0.0398$\\ 
&&\\
\textbf{N-RDSA-3-AsymBer}& $-0.2564 \pm 0.068$& $\bm{-0.2877 \pm 0.0051}$\\
 \bottomrule
 %%%%%%%%%%%%2nd ordr
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{N-SPSA-4} & $-0.0785 \pm 0.1178$ & $-0.1716 \pm 0.1339$\\
&&\\
\textbf{N-RDSA-3-Unif} &$0.0326 \pm 0.1599$ & $-0.2672 \pm 0.0299$\\ 
&&\\
\textbf{N-RDSA-3-AsymBer}& $-0.2777 \pm 0.0488$& $\bm{-0.2881 \pm 0.0012}$\\
 \bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%% NMSE for quadratic
\begin{table}
\centering
 \caption{NMSE values for quadratic objective \eqref{eq:quadratic} with and without noise: standard error from $500$ replications shown after $\pm$}
\label{tab:nmse-quadratic}
\begin{tabular}{|c|c|c|}
\toprule
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0.1$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{N-SPSA-4} & $0.9491 \pm 0.0131$ & $0.5495 \pm 0.0217$\\
&&\\
\textbf{N-RDSA-3-Unif} &$1.0073 \pm 0.0140$ & $0.1953 \pm 0.0095$\\ 
&&\\
\textbf{N-RDSA-3-AsymBer}& $0.1667 \pm 0.0095$& $\bm{0.0324 \pm 0.0007}$\\
 \bottomrule
 %%%%%%%%%%%%2nd ordr
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{N-SPSA-4} & $0.7325 \pm 0.0180$ & $0.3939 \pm 0.0230$\\
&&\\
\textbf{N-RDSA-3-Unif} &$0.9834 \pm 0.0170$ & $0.1623 \pm 0.0086$\\ 
&&\\
\textbf{N-RDSA-3-AsymBer}& $0.0686 \pm 0.0078$& $\bm{0.0316 \pm 0.0006}$\\
 \bottomrule
\end{tabular}
\end{table}


\newcommand{\errorband}[5][]{ % x column, y column, error column, optional argument for setting style of the area plot
\pgfplotstableread[col sep=comma, skip first n=2]{#2}\datatable
    % Lower bound (invisible plot)
    \addplot [draw=none, stack plots=y, forget plot] table [
        x={#3},
        y expr=\thisrow{#4}-2*\thisrow{#5}
    ] {\datatable};

    % Stack twice the error, draw as area plot
    \addplot [draw=none, fill=gray!40, stack plots=y, area legend, #1] table [
        x={#3},
        y expr=4*\thisrow{#5}
    ] {\datatable} \closedcycle;

    % Reset stack using invisible plot
    \addplot [forget plot, stack plots=y,draw=none] table [x={#3}, y expr=-(\thisrow{#4}+2*\thisrow{#5})] {\datatable};
}

 \begin{figure}
    \centering
\tabl{c}{\scalebox{0.9}{\begin{tikzpicture}
      \begin{axis}[
	xlabel={number of simulations},
	ylabel={Normalized loss},
       legend entries={
	 ,N-SPSA-4,,N-SPSA-4-IH,,N-RDSA-3-Unif,,N-RDSA-3-Unif-IH,,N-RDSA-3-AsymBer,,N-RDSA-3-AsymBer-IH },
        %legend pos=outer north east,
			legend style={
at={(0,0)},
anchor=north,at={(axis description cs:0.5,-0.2)},legend columns=2}	]
      % 2SPSA
      \errorband[red!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{1}{2}
      \addplot [thick, red,mark=square*,mark options={scale=0.7}] table [x index=0, y index=1,col sep=comma] {norloss_vs_simulations_noisycase.csv};
			%%% With IH
      % 2SPSA
      \errorband[darkgray!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{7}{8}
      \addplot [thick, darkgray,mark=*,mark options={scale=0.75}] table [x index=0, y index=7,col sep=comma] {norloss_vs_simulations_noisycase.csv};
			
      % 2RDSA-Unif
      \errorband[blue!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{3}{4}
      \addplot [thick, blue,mark=square*,mark options={scale=0.7}] table [x index=0, y index=3,col sep=comma] {norloss_vs_simulations_noisycase.csv};
			%%% With IH
      % 2RDSA-Unif
      \errorband[magenta!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{9}{10}
      \addplot [thick, magenta,mark=*,mark options={scale=0.75}] table [x index=0, y index=9,col sep=comma] {norloss_vs_simulations_noisycase.csv};


      % 2RDSA-AsymBer
      \errorband[green!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{5}{6}
      \addplot [thick, darkgreen,mark=square*,mark options={scale=0.7}] table [x index=0, y index=5,col sep=comma] {norloss_vs_simulations_noisycase.csv};
			%%% With IH	
      % 2RDSA-AsymBer
      \errorband[violet!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{11}{12}
      \addplot [thick, violet, mark=*,mark options={scale=0.75}] table [x index=0, y index=11,col sep=comma] {norloss_vs_simulations_noisycase.csv};
      \end{axis}
      \end{tikzpicture}}\\}
			\caption{Normalized loss vs. number of simulations for fourth-order loss \eqref{eq:4thorder} with $\sigma=0.1$ for N-SPSA-4, N-RDSA-3-Unif and N-RDSA-3-AsymBer algorithms with/without improved Hessian estimation: bands around the curves represent standard error from $500$ replications. }
      \label{fig:norlossvssims} 
\end{figure} 


Tables \ref{tab:norloss-4thf}--\ref{tab:norloss-quadratic} present the normalized loss values observed for the three algorithms - N-SPSA-4, N-RDSA-3-Unif and N-RDSA-3-AsymBer - with/without improved Hessian estimation scheme and for the fourth-order and quadratic loss functions, respectively. Table \ref{tab:nmse-quadratic} presents the NMSE values obtained for the aforementioned algorithms with the quadratic loss. The results in Tables \ref{tab:norloss-4thf}--\ref{tab:nmse-quadratic} are obtained after running all the algorithms with a budget of $10000$ function evaluations.
 Figure \ref{fig:norlossvssims} plots the normalized loss as a function of the simulation budget with the fourth-order loss objective with $\sigma=0.1$ (see Figures \ref{fig:norlossvssimsNoiseless}--\ref{fig:norlossvssimsNoiseless} in the appendix for similar results with $\sigma = 0$ for fourth-order loss and $\sigma=0.1$ for quadratic loss). From the results in Tables \ref{tab:norloss-4thf}--\ref{tab:nmse-quadratic} and Fig \ref{fig:norlossvssims}, we make the following observations:
 
\textit{\textbf{Observation 1:} Among N-RDSA-3 schemes, N-RDSA-3-IH performs better than regular N-RDSA-3, for both perturbation choices.}

\textit{\textbf{Observation 2:} N-RDSA-3-IH variants outperform both N-SPSA-4 and N-SPSA-4-IH, with N-RDSA-3-IH-AsymBer performing the best overall.}





\section{Conclusions}
\label{sec:conclusions}
We presented an improved Hessian estimation scheme for the second-order random directions stochastic approximation (N-RDSA-3) algorithm \cite{prashanth2015rdsa}. 
The proposed scheme was shown to be provably convergent to the true Hessian.
 %and as a special case, for a quadratic objective resulted in a  convergence rate that is on par with the corresponding rate for 2SPSA with Hessian estimation improvements (2SPSA-IH) \cite{spall-jacobian}. 
The advantage with N-RDSA-3-IH is that it requires only 75\% of the simulation cost per-iteration for N-SPSA-4 with Hessian estimation improvements (N-SPSA-4-IH) \cite{spall-jacobian}. 
Numerical experiments demonstrated that N-RDSA-3-IH outperforms both N-SPSA-4-IH and N-RDSA-3 without the improved Hessian estimation scheme.
\section*{Appendix}
\begin{figure}[h]
    \centering
		\begin{tabular}{c}
		\begin{subfigure}{0.5\textwidth}
\tabl{c}{\scalebox{0.8}{\begin{tikzpicture}
      \begin{axis}[
	xlabel={number of simulations},
	ylabel={Normalized loss},
       legend entries={
	 ,N-SPSA-4,,N-SPSA-4-IH,,N-RDSA-3-Unif,,N-RDSA-3-Unif-IH,,N-RDSA-3-AsymBer,,N-RDSA-3-AsymBer-IH },
        %legend pos=outer north east,
			legend style={
at={(0,0)},
anchor=north,at={(axis description cs:0.5,-0.2)},legend columns=2}	]
      % 2SPSA
      \errorband[red!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{1}{2}
      \addplot [thick, red,mark=square*,mark options={scale=0.7}] table [x index=0, y index=1,col sep=comma] {norloss_vs_simulations.csv};
			%%% With IH
      % 2SPSA
      \errorband[darkgray!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{7}{8}
      \addplot [thick, darkgray,mark=*,mark options={scale=0.75}] table [x index=0, y index=7,col sep=comma] {norloss_vs_simulations.csv};
			
      % N-RDSA-3-Unif
      \errorband[blue!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{3}{4}
      \addplot [thick, blue,mark=square*,mark options={scale=0.7}] table [x index=0, y index=3,col sep=comma] {norloss_vs_simulations.csv};
			%%% With IH
      % 2RDSA-Unif
      \errorband[magenta!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{9}{10}
      \addplot [thick, magenta,mark=*,mark options={scale=0.75}] table [x index=0, y index=9,col sep=comma] {norloss_vs_simulations.csv};


      % 2RDSA-AsymBer
      \errorband[green!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{5}{6}
      \addplot [thick, darkgreen,mark=square*,mark options={scale=0.7}] table [x index=0, y index=5,col sep=comma] {norloss_vs_simulations.csv};
			%%% With IH	
      % 2RDSA-AsymBer
      \errorband[violet!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{11}{12}
      \addplot [thick, violet, mark=*,mark options={scale=0.75}] table [x index=0, y index=11,col sep=comma] {norloss_vs_simulations.csv};
      \end{axis}
      \end{tikzpicture}}\\}
			\caption{Fourth-order loss \eqref{eq:4thorder} with $\sigma=0$. }
      \label{fig:norlossvssimsNoiseless} 
\end{subfigure} 
\\
\\
%%%%%%% Norloss-Quadratic-noisy %%%%%%%%%%%%%%
\begin{subfigure}{0.5\textwidth}
\tabl{c}{\scalebox{0.8}{\begin{tikzpicture}
      \begin{axis}[
	xlabel={number of simulations},
	ylabel={Normalized loss},
       legend entries={
	 ,N-SPSA-4,,N-SPSA-4-IH,,N-RDSA-3-Unif,,N-RDSA-3-Unif-IH,,N-RDSA-3-AsymBer,,N-RDSA-3-AsymBer-IH },
        %legend pos=outer north east,
			legend style={
at={(0,0)},
anchor=north,at={(axis description cs:0.5,-0.2)},legend columns=2}	]
      % 2SPSA
      \errorband[red!50!white, opacity=0.3]{quad_norloss_vs_simulations_noisycase.csv}{0}{1}{2}
      \addplot [thick, red,mark=square*,mark options={scale=0.7}] table [x index=0, y index=1,col sep=comma] {quad_norloss_vs_simulations_noisycase.csv};
			%%% With IH
      % 2SPSA
      \errorband[darkgray!50!white, opacity=0.3]{quad_norloss_vs_simulations_noisycase.csv}{0}{7}{8}
      \addplot [thick, darkgray,mark=*,mark options={scale=0.75}] table [x index=0, y index=7,col sep=comma] {quad_norloss_vs_simulations_noisycase.csv};
			
      % N-RDSA-3-Unif
      \errorband[blue!50!white, opacity=0.3]{quad_norloss_vs_simulations_noisycase.csv}{0}{3}{4}
      \addplot [thick, blue,mark=square*,mark options={scale=0.7}] table [x index=0, y index=3,col sep=comma] {quad_norloss_vs_simulations_noisycase.csv};
			%%% With IH
      % 2RDSA-Unif
      \errorband[magenta!50!white, opacity=0.3]{quad_norloss_vs_simulations_noisycase.csv}{0}{9}{10}
      \addplot [thick, magenta,mark=*,mark options={scale=0.75}] table [x index=0, y index=9,col sep=comma] {quad_norloss_vs_simulations_noisycase.csv};


      % 2RDSA-AsymBer
      \errorband[green!50!white, opacity=0.3]{quad_norloss_vs_simulations_noisycase.csv}{0}{5}{6}
      \addplot [thick, darkgreen,mark=square*,mark options={scale=0.7}] table [x index=0, y index=5,col sep=comma] {quad_norloss_vs_simulations_noisycase.csv};
			%%% With IH	
      % 2RDSA-AsymBer
      \errorband[violet!50!white, opacity=0.3]{quad_norloss_vs_simulations_noisycase.csv}{0}{11}{12}
      \addplot [thick, violet, mark=*,mark options={scale=0.75}] table [x index=0, y index=11,col sep=comma] {quad_norloss_vs_simulations_noisycase.csv};
      \end{axis}
      \end{tikzpicture}}\\}
			\caption{Quadratic loss \eqref{eq:quadratic} with $\sigma=0.1$. }
      \label{fig:QuadnorlossvssimsNoisy} 
\end{subfigure} 
		\end{tabular}
			\caption{Normalized loss vs. number of simulations in two different loss settings for all the algorithms. }
      \label{fig:extras} 
		\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{IEEEtran}
\bibliography{reference}
\end{document}





