%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{macros}


\title{\LARGE \bf
Improved Hessian estimation for adaptive \\random directions stochastic approximation
}



\author{D. Sai Koti Reddy$^\dagger$, Prashanth L A$^\sharp$, Shalabh Bhatnagar$^\ddag$
\thanks{
$^\dagger$ Department of Computer Science and Automation,
Indian Institute of Science, Bangalore,
E-Mail: danda.reddy@csa.iisc.ernet.in}
\thanks{
$^\sharp$ Institute for Systems Research, University of Maryland, College Park, Maryland,
E-Mail: prashanth@isr.umd.edu.
}
\thanks{
$^\ddag$ Department of Computer Science and Automation,
Indian Institute of Science, Bangalore,
E-Mail: shalabh@csa.iisc.ernet.in
}
}



\begin{document}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
to be done


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
We consider the following \textit{simulation optimization} problem:
\begin{align}
\mbox{Find } x^* = \arg\min_{x \in \R^N} f(x). \label{eq:pb}
\end{align}
The aim is find an algorithm that solves \eqref{eq:pb} given only noise-corrupted measurements of the objective $f$.
A natural solution approach is to devise an algorithm that incrementally updates the parameter, say $x_n$, in the descent direction using the gradient and/or Hessian of the objective $f$. However, gradient/Hessian of $f$ are unavailable directly and need to be estimated from samples of $f$. 
To overcome this, we adopt the simultaneous perturbation (SP) approach - a popular and efficient idea esp. in high dimensional problems - see \cite{bhatnagar-book} for a comprehensive treatment of this subject matter. 

Simultaneous perturbation stochastic approximation (SPSA) is a popular SP method. The first-order SPSA algorithm, henceforth referred to as 1SPSA, was proposed in \cite{spall}.  A closely related idea is random directions stochastic approximation (RDSA) \cite[pp.~58-60]{kushcla}. The gradient estimate in RDSA differs from that in SPSA, both in construction as well as in the choice of random perturbations.  In \cite{kushcla}, the random perturbations for 1RDSA were generated by picking uniformly on the surface of a sphere and the resulting 1RDSA scheme was shown to be highly inferior to 1SPSA from a asymptotic convergence rate viewpoint - see \cite{chin1997comparative}.  Recent work in \cite{prashanth2015rdsa} bridged the gap between 1RDSA and 1SPSA by incorporating random perturbations based on an asymmetric Bernoulli distribution, with 1SPSA still marginally better than 1RDSA. 
On the other hand, results in \cite{prashanth2015rdsa} show that a second order RDSA approach (referred to as 2RDSA hereafter) can considerably outperform the corresponding SPSA algorithm \cite{spall_adaptive} (referred to as 2SPSA hereafter).  

Our work in this paper is centered on improving the 2RDSA scheme of \cite{prashanth2015rdsa} by \\
\begin{inparaenum}[\bfseries (i)]
\item reducing the error in the Hessian estimate through a feedback term; and\\
\item optimizing the step-sizes used in the Hessian estimation recursion, again with the objective of improving the quality of the Hessian estimate.
\end{inparaenum}

Items (i) and (ii) are inspired by the corresponding improvements to the Hessian estimation recursion in 2SPSA - see \cite{spall-jacobian}. While item (ii) is a relatively straightforward migration to 2RDSA setting, item (i) is a non-trivial contribution, primarily because the Hessian estimate in 2RDSA is entirely different from that in 2SPSA and the feedback term that we incorporate in 2RDSA to improve the Hessian estimate neither correlates to that in 2SPSA nor follows from the analysis in \cite{spall-jacobian}. Further, we show empirically that 2RDSA with feedback that we propose here outperforms both 2SPSA with feedback and 2RDSA without feedback. Our contribution is important because 2RDSA with feedback, like 2RDSA, has lower simulation cost per iteration than 2SPSA and unlike 2RDSA, has an improved Hessian estimation scheme.


\section{SECOND-ORDER RDSA WITH IMPROVED HESSIAN ESTIMATION (2RDSA-IH)}
The second-order RDSA with improved Hessian estimate performs an update iteration as follows:
\begin{align}
\label{eq:e2rdsa}
x_{n+1} = x_n - a_n \Upsilon(\overline H_n)^{-1}\widehat\nabla f(x_n), \\
\overline H_n = (1-w_{n})  \overline H_{n-1} + w_{n} ( \widehat H_n - \widehat \Psi_n),\label{eq:2rdsa-H}
\end{align}
where $\widehat\nabla f(x_n)$ is the estimate of $\nabla f(x_n)$, 
 %and this corresponds to  the uniform  and   asymmetric Bernoulli variant gradient estimates proposed in \cite{prashanth2015rdsa}.
$\overline H_n$ is an estimate of the true Hessian ${\nabla}^2 f(\cdot)$, $\Upsilon(\cdot)$ projects any matrix onto the set of positive definite matrices and $\{a_n, n\ge 0\}$ is a step-size sequence that satisfies standard stochastic approximation conditions.  

The recursion \eqref{eq:e2rdsa} is identical to that in 2RDSA, while the Hessian estimation recursion \eqref{eq:2rdsa-H} differs as follows:\\
\begin{inparaenum}[\bfseries (i)]
\item  $\widehat \Psi_n$ is a zero-mean feedback term that reduces the error in Hessian estimate; and\\
\item $w_n$ is a general step-size that we optimize to improve the Hessian estimate.
\end{inparaenum}

On the other hand, $\widehat H_n$ is identical to that in 2RDSA, i.e., it estimates the true Hessian in each iteration using $3$ function evaluations. %Further, $\widehat H_0 = I$. 
For the sake of completeness, we provide below the construction for $\widehat\nabla f(x_n)$ and $\widehat H_n$ using asymmetric Bernoulli perturbations, before we present the feedback term that reduces the error in $\widehat H_n$.

\paragraph{Function evaluations}
Let $y_n$, $y_n^+$ and $y_n^-$ denote the function evaluations at $x_n$, $x_n+\delta_n d_n$ and $x_n - \delta_n d_n$ respectively, i.e., 
$y_n = f(x_n) + \xi_n$, $y_n^+ = f(x_n+\delta_n d_n) + \xi_n^+$ and 
$y_n^- = f(x_n-\delta_n d_n) + \xi_n^-$,
where the noise terms $\xi_n, \xi_n^+, \xi_n^-$ satisfy $\E\left[\left.\xi_n^+ + \xi_n^- - 2\xi_n \right| \F_n\right] = 0$ with $\F_n = \sigma(x_m,m\le n)$ denoting the underlying sigma-field.

\paragraph{Gradient estimate}
The RDSA estimate of the gradient $\nabla f(x_n)$ is given by
\begin{align}
\label{eq:grad-ber}
\widehat\nabla f(x_n) = \frac1{1+\epsilon} d_n \left[ \dfrac{y_n^+ - y_n^-}{2\delta_n}\right],
\end{align}
where the perturbations $d_n^i$, $i=1,\ldots,N$ are i.i.d. and distributed  as follows: 
\begin{equation}
\label{eq:det-proj}
 d_n^i =
  \begin{cases}
   -1 &  \text{ w.p. } \dfrac{(1+\epsilon)}{(2+\epsilon)}, \\
   1+\epsilon &  \text{ w.p. } \dfrac{1}{(2+\epsilon)},
  \end{cases}
\end{equation}
where $\epsilon>0$ is a constant that can be chosen to be arbitrarily small.
%Note that $E d_n^i = 0$, $E (d_n^i)^2 = 1+\epsilon$ and $E (d_n^i)^4 = \dfrac{(1+\epsilon)(1+(1+\epsilon)^3)}{(2+\epsilon)}$.

\paragraph{Hessian estimate}

\begin{align}
\label{eq:2rdsa-estimate-ber}
&\widehat H_n = M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right), \text{ where }\\
& M_n =
\left[
\begin{array}{ccc}
\frac{1}{\kappa}\left((d_n^1)^2\!-(1+\epsilon)\right) & \cdots & \frac{1}{2(1+\epsilon)^2}d_n^1 d_n^N\\
\frac{1}{2(1+\epsilon)^2}d_n^2 d_n^1  &  \cdots & \frac{1}{2(1+\epsilon)^2}d_n^2 d_n^N\\
\frac{1}{2(1+\epsilon)^2}d_n^N d_n^1 & \cdots &  \frac{1}{\kappa}\left((d_n^N)^2-(1+\epsilon)\right) \\
\end{array}
\right],\nonumber
\end{align}
where $\kappa = \tau \left(1- \dfrac{(1+\epsilon)^2}{\tau}\right)$ and $\tau = E (d_n^i)^4= \dfrac{(1+\epsilon)(1+(1+\epsilon)^3)}{(2+\epsilon)}$, for any $i=1,\ldots,N$. 
%%%%%% Feedback
\paragraph{Feedback term $\widehat \Psi_n$}
The Hessian estimate $\widehat H_n$ can be simplified as follows: 
\begin{align}
\widehat H_n & = M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right) \nonumber \\
& =  M_n \left[\left(\dfrac{f(x_n+\delta_n d_n) + f(x_n-\delta_n d_n) - 2 f(x_n)}{\delta_n^2}\right) \right.\nonumber \\&\hspace{10em}+\left. \left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2}\right)\right] \nonumber \\
& = M_n \left(d_n\tr \nabla^2 f(x_n) d_n +  O(\delta_n^2) + \left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2} \right)\right)\label{eq:h0}
%& = M_n \left(d_n\tr \nabla^2 f(x_n) d_n +  O(\delta_n^2) + O(\delta_n^{-2})\right). \label{eq:h0}
\end{align}
Analyzing the first term on the RHS above, we obtain
\begin{align}
&\E[M_n \left(d_n\tr \nabla^2 f(x_n) d_n\right) \mid \F_n] =   \E\left[\left. M_n \times\right.\right.\nonumber\\
&\left.\left.\left(\sum\limits_{i=1}^{N-1} (d_n^i)^2 \nabla^2_{ii} f(x_n) + 2\sum\limits_{i=1}^N\sum\limits_{j=i+1}^N d_n^i d_n^j \nabla^2_{ij} f(x_n) \right)\right| \F_n\right]. \label{eq:h1}
\end{align}
In analyzing the $l$th diagonal term of above expression, the following zero-mean term appears (see the proof of Lemma 4 in \cite{prashanth2015rdsa}):
\begin{align*}
& \E\left[\left.\dfrac{45}{4\eta^4} \left((d_n^l)^2-\frac{\eta^2}{3}\right) \left(2\sum\limits_{i=1}^{N-1}\sum\limits_{j=i+1}^N d_n^i d_n^j \nabla^2_{ij} f(x_n)\right)\right| \F_{n}\right]\nonumber \\ &= 0.
\end{align*}
The term on the LHS above, denoted by $\Psi_{n}^{1}(\nabla^2 f(x_n))$, can be written in matrix form as follows: 
\begin{align}\label{eq:digmat}
\Psi_{n}^{1}(\nabla^2 f(x_n)) = [M_n]_{D}\left(d_{n}\tr \, [\nabla^2 f(x_n)]_{N} \, d_{n}\right).
\end{align}
where for a matrix $[.]_{D}$ refers to keeping all its diagonal elements intact and making all its remaining elements zero. vice versa for $[.]_{N}$.

In analyzing the off-diagonal term($(k,l)$ where $k < l$) of \eqref{eq:h1}, the following zero-mean term appears:
\begin{align}
& \dfrac{9}{2\eta^4} \E\left[\left.d_n^k d_n^l   \left(\sum\limits_{i=1}^N (d_n^i)^2 \nabla^2_{ii} f(x_n)\right)\right| \F_n \right] = 0. 
\end{align}
The term on the LHS above, denoted by $\Psi_{n}^{2}(\nabla^2 f(x_n))$, can be written in matrix form as follows: 
\begin{align}\label{eq:ndmat}
\Psi_{n}^{2}(\nabla^2 f(x_n)) = [M_n]_{N}\left(d_{n}\tr \, [\nabla^2 f(x_n)]_{D} \, d_{n}\right).
\end{align}
From the foregoing, the per-iteration Hessian estimate $\widehat H_n$ can be re-written as follows:
\begin{align}
 \E\left[\left.\widehat H_n\right| \F_n\right] = & \nabla^2 f(x_n) + \E\left[\left.\Psi_{n}(\nabla^2 f(x_n))\right| \F_n\right]  +  O(\delta_n^2)\nonumber\\
&+\E\left[\left.\left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2} \right)\right| \F_n\right], \label{eq:hnhat}
\end{align}
where, for any matrix $H$, 
\begin{align}
&\Psi_{n}(H) = \Psi_{n}^{1}(H) + \Psi_{n}^{2}(H)\nonumber\\
&\qquad= [M_n]_{D}\left(d_{n}\tr \, [H]_{N} \, d_{n}\right) +  [M_n]_{N}\left(d_{n}\tr \, [H]_{D} \, d_{n}\right).
\end{align}
In the RHS of \eqref{eq:hnhat}, it is easy to see that that the second term involving $\Psi_{n}$ and the last term involving the noise are zero-mean. Moreover, since the noise is bounded by assumption, the last term in \eqref{eq:hnhat} vanishes asymptotically at the rate $O(\delta_n^{-2})$. So, the error in estimating the Hessian is due to the second term, which involves the perturbations $d_n$. This motivates the term $\widehat \Psi_n$ in the update rule \eqref{eq:e2rdsa}. 
 %and using the fact that $\E\left[\left.\xi_n^+ + \xi_n^- - 2\xi_n \right| \F_n\right] = 0$


Given that we operate in a simulation optimization setting, which implies $\nabla^2 f$ is not known, we construct the feedback term $\widehat \Psi_n$ in \eqref{eq:e2rdsa} by using $\overline H_{n-1}$ as a proxy for $\nabla^2 f$, i.e.,
\begin{align}
\widehat \Psi_n = \Psi_{n} (\overline H_{n-1}).
\end{align}

\paragraph{Optimizing step-sizes $w_n$}
Unlike the feedback term, adapting the idea of optimizing the step-sizes for 2RDSA is relatively straightforward from the corresponding approach for 2SPSA in \cite{spall-jacobian}. The difference here is that there exist only one $N$-dimensional perturbation vector $d_n$ in our setting, while 2SPSA had two $N$-dimensional vectors. This in turn implies that only the perturbation constant $\delta_n$ is needed in optimizing $w_n$.

The optimal choice for $w_n$ in \eqref{eq:2rdsa-H} is the following:
\begin{align}
\label{eq:wieghts}
w_i  = \delta_i^{4}/\sum\limits_{j=0}^{i} \delta_j^{4}.
\end{align}
The main idea behind above choice is provided below.
From \eqref{eq:hnhat}, we can infer that
\begin{align*}
\E \l \widehat H_n \r^2 \le \dfrac{C}{\delta_n^2} \text{ for some } C<\infty. 
\end{align*} 
This is because the third term in \eqref{eq:hnhat} vanishes asymptotically, while the fourth term there dominates asymptotically. Moreover, the noise factors in the fourth term in \eqref{eq:hnhat} are bounded above due to (C9) and independent of $n$, leaving the $\delta_n^2$ term in the denominator there. 

So, the optimization problem to be solved at instant $n$ is as follows:
\begin{align}
\sum \limits_{i=0}^{n} (\tilde w_k)^2 \delta_i^{-4}, \text{ subject to} \label{eq:wn-opt}\\
\tilde w_i \geq 0 ~\forall i \text{ and }\sum \limits_{i=0}^{n} \tilde w_i = 1.
\end{align}
In the above, $\tilde w_i$ are equivalent to the step-sizes $w_i$ in the following sense:
\begin{align}
\label{eq:hess}
\overline H_n = \sum\limits_{i=0}^{n} w_k^{(n)}(\widehat H_i -\widehat \Psi_i).
\end{align}
The solution to \eqref{eq:wn-opt} is achieved for $\tilde w_i^* = \delta_i^{4}/\sum \limits_{j=0}^{n} \delta_j^{4}, i=1,\ldots,n$. The optimal choice $\tilde w_i^*$ can be translated onto the step-sizes $w_i$, leading to \eqref{eq:wieghts}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}\textbf{\textit{(Uniform perturbations)}}
In \cite{prashanth2015rdsa}, the authors suggest two alternatives for the distribution of random perturbations $d_n$: the asymmetric Bernoulli, which we described earlier and uniform that we outline next.

Choose $d_n^i$, $ i=1,\ldots,N$ to be i.i.d. $U[-\eta,\eta]$ for some $\eta>0$, where $U[-\eta,\eta]$ denotes the uniform distribution on the interval $[-\eta,\eta]$.
Then, the RDSA estimate of the gradient is given by
\begin{align}
\label{eq:grad-unif}
\widehat\nabla f(x_n) = \frac3{\eta^2} d_n \left[ \dfrac{y_n^+ - y_n^-}{2\delta_n}\right].
\end{align}

The Hessian estimate is given by
\begin{align}
\label{eq:2rdsa-estimate-unif}
&\widehat H_n =  M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right), \text{ where } \\
& M_n =
\dfrac{9}{2\eta^4}\left[
\begin{array}{cccc}
\frac{5}{2}\left((d_n^1)^2-\frac{\eta^2}{3}\right) & \cdots & d_n^1 d_n^N\\
d_n^2 d_n^1  &  \cdots & d_n^2 d_n^N\\
d_n^N d_n^1 & \cdots &  \frac{5}{2}\left((d_n^N)^2-\frac{\eta^2}{3}\right) \\
\end{array}
\right].\nonumber
\end{align}

The feedback term that we propose can be easily extended to the case of uniform perturbation, by using the $M_n$ as defined above instead of that for the asymmetric Bernoulli case.
\end{remark}
%Henceforth, we shall refer to algorithm \eqref{eq:2rdsa}--\eqref{eq:2rdsa-H} with Hessian estimate \eqref{eq:2rdsa-estimate} as 2RDSA.


\subsection{Main results}
\label{sec:2rdsa-results}
 $\mathcal{F}_n = \sigma(x_m,m\le n)$ denotes the underlying sigma-field. 
We make the following assumptions that are similar to those in \cite{spall_adaptive}:
\begin{enumerate}[label=(\textbf{C\arabic*})]
\item  The function
$f$ is four-times differentiable\footnote{Here $\nabla^4 f(x) = \dfrac{\partial^4 f (x)}{\partial x\tr \partial x\tr \partial x\tr \partial x\tr}$ denotes the fourth derivate of $f$ at $x$ and $\nabla^4_{i_1 i_2 i_3 i_4} f(x)$ denotes the $(i_1 i_2 i_3 i_4)$th entry of $\nabla^4 f(x)$, for $i_1, i_2, i_3,i_4=1,\ldots, N$.} with $\left|\nabla^4_{i_1 i_2 i_3 i_4} f(x) \right| < \infty$, for $i_1, i_2, i_3,i_4=1,\ldots, N$ and for all $x\in \R^N$. 

%\item  For some $\rho>0$  and almost all $x_n$, the function $f$ is four-times differentiable with a uniformly (in $n$) bounded fourth derivative for all $x$ such that $\left\| x_n - x\right\| \le \rho$. 

\item For each $n$ and all $x$, there exists a $\rho>0$ not dependent on $n$ and $x$, such that $(x-x^*)\tr \bar f_n(x) \ge \rho \left\| x_n - x\right\|$, where $\bar f_n(x) = \Upsilon(\overline H_n)^{-1} \nabla f(x)$.

\item $\{\xi_n, \xi_n^+,\xi_n^-, n=1,2,\ldots\}$ satisfy $\E\left[\left. \xi_n^+ + \xi_n^- - 2 \xi_n \right| \F_n\right] = 0$, for all $n$. 

\item  Same as (A4). %$\{d_n^i, i=1,\ldots,N, n=1,2,\ldots\}$ are i.i.d. and independent of $\F_n$.

\item Same as (A5).

\item For each $i=1,\ldots,N$ and any $\rho>0$, 
$P(\{ \bar f_{ni} (x_n) \ge 0 \text{ i.o}\} \cap \{ \bar f_{ni} (x_n) < 0 \text{ i.o}\} \mid \{ |x_{ni} - x^*_i| \ge \rho\quad \forall n\}) =0.$

\item The operator $\Upsilon$ satisfies $\delta_n^2 \Upsilon(H_n)^{-1} \rightarrow 0$ a.s. and  $E(\left\| \Upsilon(H_n)^{-1}\right\|^{2+\zeta}) \le \rho$ for some $\zeta, \rho>0$.

\item For any $\tau >0$ and nonempty $S \subseteq \{1,\ldots,N\}$, there exists a $\rho'(\tau,S)>\tau$ such that 
$$ \limsup_{n\rightarrow \infty} \left| \dfrac{\sum_{i \notin S} (x-x^*)_i \bar f_{ni}(x)}{\sum_{i \in S} (x-x^*)_i \bar f_{ni}(x)}               \right| < 1 \text{ a.s.}$$
for all $|(x-x^*)_i| < \tau$ when $i \notin S$ and $|(x-x^*)_i| \ge \rho'(\tau,S)$ when $i\in S$.
\item For some $\alpha_0, \alpha_1>0$ and for all $n$, $\E {\xi_n}^{2} \le \alpha_0$, $\E {\xi_n^{\pm}}^{2} \le \alpha_0$, $\E f(x_n)^{2} \le \alpha_1$ and $\E f(x_n\pm \delta_n d_n)^{2} \le \alpha_1$. 
\item  $\sum_n \frac{1}{(n+1)^{2}\delta_n^{2}} < \infty$.
\end{enumerate}
\begin{lemma}(\textbf{Bias in Hessian estimate})
\label{lemma:2rdsa-bias}
Under (C1)-(C10), with $\widehat H_n$ defined according to either \eqref{eq:2rdsa-estimate-unif} or \eqref{eq:2rdsa-estimate-ber}, we have a.s. that\footnote{Here $\widehat H_n(i,j)$ and $\nabla^2_{ij}f(\cdot)$ denote the $(i,j)$th entry in the Hessian estimate $\widehat H_n$ and the true Hessian $\nabla^2 f(\cdot)$, respectively.}, for $i,j = 1,\ldots,N$,
\begin{align}
\left|\E\left[
\left. \widehat H_n(i,j) \right| \F_n \right] - \nabla^2_{ij} f(x_n)\right| = O(\delta_n^2).
\end{align} 
\end{lemma}
\begin{proof}
See Lemma 4 in \cite{prashanth2015rdsa}.
\end{proof}

\begin{theorem}(\textbf{Strong Convergence of Hessian})
\label{thm:2rdsa-H}
Under (C1)-(C10), we have that 
$$\overline H_n \rightarrow \nabla^2 f(x^*) \text{ a.s. as } n\rightarrow \infty.$$ 
In the above, $\overline H_n$ is updated according to \eqref{eq:2rdsa-H} and $\widehat H_n$ defined according to either \eqref{eq:2rdsa-estimate-ber} or \eqref{eq:2rdsa-estimate-unif}. 
\end{theorem}
\begin{proof}
to be done.
\end{proof}

\section{SIMULATION EXPERIMENTS}
\label{sec:expts}
\subsection{Implementation}
We test the performance of 2RDSA-IH, 2RDSA and 2SPSA, with/without improved Hessian estimation. For this purpose, we use the following two loss functions in $N=10$ dimensions:
\paragraph{Quadratic loss}
\begin{align}
f(x) = x\tr A x + b\tr x,\label{eq:quadratic}
\end{align} 
The optimum $x^*$ for the above $f$ is such that each coordinate of $x^*$ is $-0.9091$, with $f(x^*) = -4.55$

\paragraph{Fourth-order loss}
\begin{align} 
f(x) = x\tr A\tr A x + 0.1 \sum_{j=1}^N (Ax)^3_j + 0.01 \sum_{j=1}^N (Ax)^4_j,\label{eq:4thorder}
 \end{align} 
The optimum $x^*$ for above $f$ is $x^*=0$, with $f(x^*) = 0$. 

In both functions, $A$ is such that $NA$ is an upper triangular matrix with each entry one, $b$ is the $N$-dimensional vector of ones and the noise structure is similar to that used in \cite{spall_adaptive}. For any $x$, the noise is $[x\tr, 1]z$, where $z \approx \N(0,\sigma^2 I_{11\times11})$. We perform experiments for noisy as well as noise-less settings, with $\sigma=0.1$ for the noisy case. 



For all algorithms, we set $\delta_n = 3.8/n^{0.101}$ and $a_n = 1/n^{0.6}$. These choices have been used  for 2SPSA implementations before (see \cite{spall_adaptive}) and have demonstrated good finite-sample performance empirically, while satisfying the theoretical requirements needed for asymptotic convergence. 2SPSA algorithm uses Bernoulli $\pm 1$-valued perturbations, while 2RDSA/2RDSA-IH come in two variants - one that uses $U[-1,1]$ distributed perturbations (referred to as 2RDSA-Unif/2RDSA-IH-Unif) and the other that uses asymmetric Bernoulli perturbations (referred to as 2RDSA-AsymBer/2RDSA-IH-AsymBer). For all the algorithms, the initial point $x_0$ is the $N$-dimensional vector of ones.  For both 2SPSA and 2RDSA/2RDSA-IH, an initial $20\%$ of the the simulation budget was used up by 1SPSA/1RDSA and the resulting iterate was used to initialize 2SPSA/2RDSA. The distribution parameter $\epsilon$ is set to $0.0001$ for 2RDSA and to $0.01$ for 1RDSA. 

\subsection{Results}
We use normalized loss and normalized MSE (NMSE) as performance metrics for evaluating the algorithms. 
NMSE is the ratio $\l x_{n_\text{end}} - x^* \r^2 / \l x_0 - x^*\r^2$, while normalized loss is the ratio $f(x_{n_\text{end}})/f(x_0)$.  Here $n_\text{end}$ denotes the iteration number when the algorithm stopped updating its parameter. Note that $n_\text{end}$ is a function of the simulation budget. 2RDSA/2RDSA-IH use only three simulations per-iteration and hence, $n_\text{end}$ is $1/3$rd of the simulation budget, while it is $1/4$th of the simulation budget for 2SPSA, since the latter algorithm uses four simulations per-iteration. 

Tables \ref{tab:norloss-4thf}--\ref{tab:norloss-quadratic} present the normalized loss values observed for the three algorithms - 2SPSA, 2RDSA-Unif and 2RDSA-AsymBer - with/without feedback and for the fourth-order and quadratic loss functions, respectively. Further, Table \ref{tab:nmse-quadratic} presents the NMSE values obtained for the aforementioned algorithms with the quadratic loss. From the results in Tables \ref{tab:norloss-4thf}--\ref{tab:nmse-quadratic}, we make the following observations:
 
\textit{\textbf{Observation 1:} Among 2RDSA schemes, 2RDSA-IH performs better than regular 2RDSA, for both perturbation choices.}

\textit{\textbf{Observation 2:} 2RDSA-IH variants outperform both 2SPSA and 2SPSA-IH, with 2RDSA-IH-AsymBer performing the best overall.}

\begin{table}
\centering
 \caption{Normalized loss values for fourth-order  objective \eqref{eq:4thorder} with and without noise: standard error from $500$ replications shown after $\pm$}
\label{tab:norloss-4thf}
\begin{tabular}{|c|c|c|}
\toprule
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0.1$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $0.0132 \pm 0.0267$ & $0.0104 \pm 0.0355$\\
&&\\
\textbf{2RDSA-Unif} &$0.0115 \pm 0.0214$ & $0.0271 \pm 0.0538$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $0.0471 \pm 0.021$& $\bm{0.0099 \pm 0.0014}$\\
 \bottomrule
 %%%%%%%%%%%%2nd ordr
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $0.0795 \pm 0.0234$ & $0.0628 \pm 0.0234$\\
&&\\
\textbf{2RDSA-Unif} &$0.0813 \pm 0.0275$ & $0.0214 \pm 0.00376$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $0.0199 \pm 0.0114$& $\bm{0.0098 \pm 0.00147}$\\
 \bottomrule
\end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%% Quadratic
\begin{table}
\centering
 \caption{Normalized loss values for quadratic objective \eqref{eq:quadratic} with and without noise: standard error from $500$ replications shown after $\pm$}
\label{tab:norloss-quadratic}
\begin{tabular}{|c|c|c|}
\toprule
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0.1$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $-0.0062 \pm 0.1164$ & $-0.1229 \pm 0.1374$\\
&&\\
\textbf{2RDSA-Unif} &$0.0485 \pm 0.1465$ & $-0.259 \pm 0.0398$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $-0.2564 \pm 0.068$& $\bm{-0.2877 \pm 0.0051}$\\
 \bottomrule
 %%%%%%%%%%%%2nd ordr
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $-0.0785 \pm 0.1178$ & $-0.1716 \pm 0.1339$\\
&&\\
\textbf{2RDSA-Unif} &$0.0326 \pm 0.1599$ & $-0.2672 \pm 0.0299$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $-0.2777 \pm 0.0488$& $\bm{-0.2881 \pm 0.0012}$\\
 \bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%% NMSE for quadratic
\begin{table}
\centering
 \caption{NMSE values for quadratic objective \eqref{eq:quadratic} with and without noise: standard error from $500$ replications shown after $\pm$}
\label{tab:nmse-quadratic}
\begin{tabular}{|c|c|c|}
\toprule
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0.1$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $0.9491 \pm 0.0131$ & $0.5495 \pm 0.0217$\\
&&\\
\textbf{2RDSA-Unif} &$1.0073 \pm 0.0140$ & $0.1953 \pm 0.0095$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $0.1667 \pm 0.0095$& $\bm{0.0324 \pm 0.0007}$\\
 \bottomrule
 %%%%%%%%%%%%2nd ordr
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $0.7325 \pm 0.0180$ & $0.3939 \pm 0.0230$\\
&&\\
\textbf{2RDSA-Unif} &$0.9834 \pm 0.0170$ & $0.1623 \pm 0.0086$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $0.0686 \pm 0.0078$& $\bm{0.0316 \pm 0.0006}$\\
 \bottomrule
\end{tabular}
\end{table}


\newcommand{\errorband}[5][]{ % x column, y column, error column, optional argument for setting style of the area plot
\pgfplotstableread[col sep=comma, skip first n=2]{#2}\datatable
    % Lower bound (invisible plot)
    \addplot [draw=none, stack plots=y, forget plot] table [
        x={#3},
        y expr=\thisrow{#4}-2*\thisrow{#5}
    ] {\datatable};

    % Stack twice the error, draw as area plot
    \addplot [draw=none, fill=gray!40, stack plots=y, area legend, #1] table [
        x={#3},
        y expr=4*\thisrow{#5}
    ] {\datatable} \closedcycle;

    % Reset stack using invisible plot
    \addplot [forget plot, stack plots=y,draw=none] table [x={#3}, y expr=-(\thisrow{#4}+2*\thisrow{#5})] {\datatable};
}

 \begin{figure}
    \centering
\tabl{c}{\scalebox{0.97}{\begin{tikzpicture}
      \begin{axis}[
	xlabel={number of simulations},
	ylabel={NMSE},
       legend entries={
	 ,2SPSA,,2SPSA-IH,,2RDSA-Unif,,2RDSA-Unif-IH,,2RDSA-AsymBer,,2RDSA-AsymBer-IH },
        %legend pos=outer north east,
			legend style={
at={(0,0)},
anchor=north,at={(axis description cs:0.5,-0.2)},legend columns=2}	]
      % 2SPSA
      \errorband[red!50!white, opacity=0.3]{nmsevssimulations.csv}{0}{1}{2}
      \addplot [thick, red,mark=square*,mark options={scale=0.7}] table [x index=0, y index=1,col sep=comma] {nmsevssimulations.csv};
			%%% With IH
      % 2SPSA
      \errorband[darkgray!50!white, opacity=0.3]{nmsevssimulations.csv}{0}{7}{8}
      \addplot [thick, darkgray,mark=*,mark options={scale=0.75}] table [x index=0, y index=7,col sep=comma] {nmsevssimulations.csv};
			
      % 2RDSA-Unif
      \errorband[blue!50!white, opacity=0.3]{nmsevssimulations.csv}{0}{3}{4}
      \addplot [thick, blue,mark=square*,mark options={scale=0.7}] table [x index=0, y index=3,col sep=comma] {nmsevssimulations.csv};
			%%% With IH
      % 2RDSA-Unif
      \errorband[magenta!50!white, opacity=0.3]{nmsevssimulations.csv}{0}{9}{10}
      \addplot [thick, magenta,mark=*,mark options={scale=0.75}] table [x index=0, y index=9,col sep=comma] {nmsevssimulations.csv};


      % 2RDSA-AsymBer
      \errorband[green!50!white, opacity=0.3]{nmsevssimulations.csv}{0}{5}{6}
      \addplot [thick, darkgreen,mark=square*,mark options={scale=0.7}] table [x index=0, y index=5,col sep=comma] {nmsevssimulations.csv};
			%%% With IH	
      % 2RDSA-AsymBer
      \errorband[violet!50!white, opacity=0.3]{nmsevssimulations.csv}{0}{11}{12}
      \addplot [thick, violet, mark=*,mark options={scale=0.75}] table [x index=0, y index=11,col sep=comma] {nmsevssimulations.csv};
      \end{axis}
      \end{tikzpicture}}\\}
			\caption{NMSE as a function of number of simulations for 2SPSA, 2RDSA-Unif and 2RDSA-AsymBer algorithms with/without improved Hessian estimation: bands around the curves represent standard error from $500$ replications. }
      \label{fig:mcpg} 
\end{figure} 







\section{CONCLUSIONS}


\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\bibliographystyle{IEEEtran}
\bibliography{reference}



\end{document}
