%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{macros}



\title{\LARGE \bf
Improved Hessian estimation for adaptive \\random directions stochastic approximation
}



\author{D. Sai Koti Reddy$^\dagger$, Prashanth L A$^\sharp$, Shalabh Bhatnagar$^\ddag$
\thanks{
$^\dagger$ Department of Computer Science and Automation,
Indian Institute of Science, Bangalore,
E-Mail: danda.reddy@csa.iisc.ernet.in}
\thanks{
$^\sharp$ Institute for Systems Research, University of Maryland, College Park, Maryland,
E-Mail: prashanth@isr.umd.edu.
}
\thanks{
$^\ddag$ Department of Computer Science and Automation,
Indian Institute of Science, Bangalore,
E-Mail: shalabh@csa.iisc.ernet.in
}
}



\begin{document}


\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
to be done


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
We consider the following \textit{simulation optimization} problem:
\begin{align}
\mbox{Find } x^* = \arg\min_{x \in \R^N} f(x). \label{eq:pb}
\end{align}
The aim is find an algorithm that solves \eqref{eq:pb} given only noise-corrupted measurements of the objective $f$ - see Figure \ref{fig:so}.
A natural solution approach is to devise an algorithm that incrementally updates the parameter, say $x_n$, in the descent direction using the gradient and/or Hessian of the objective $f$. However, gradient/Hessian of $f$ are unavailable directly and need to be estimated from samples of $f$. 
To overcome this, we adopt the simultaneous perturbation (SP) approach - a popular and efficient idea esp. in high dimensional problems - see \cite{bhatnagar-book} for a comprehensive treatment of this subject matter. 


Simultaneous perturbation stochastic approximation (SPSA) is a popular SP method. The first-order SPSA algorithm, henceforth referred to as 1SPSA, was proposed in \cite{spall}.  A closely related idea is random directions stochastic approximation (RDSA) \cite[pp.~58-60]{kushcla}. The gradient estimate in RDSA differs from that in SPSA, both in construction as well as in the choice of random perturbations.  In \cite{kushcla}, the random perturbations for 1RDSA were generated by picking uniformly on the surface of a sphere and the resulting 1RDSA scheme was shown to be highly inferior to 1SPSA from a asymptotic convergence rate viewpoint - see \cite{chin1997comparative}.  Recent work in \cite{prashanth2015rdsa} bridged the gap between 1RDSA and 1SPSA by incorporating random perturbations based on an asymmetric Bernoulli distribution, with 1SPSA still marginally better than 1RDSA. 
On the other hand, results in \cite{prashanth2015rdsa} show that a second order RDSA approach (referred to as 2RDSA hereafter) can considerably outperform the corresponding SPSA algorithm \cite{spall_adaptive} (referred to as 2SPSA hereafter).  

Our work in this paper is centered on improving the 2RDSA scheme of \cite{prashanth2015rdsa} by \\
\begin{inparaenum}[\bfseries (i)]
\item reducing the error in the Hessian estimate through a feedback term; and\\
\item optimizing the step-sizes used in the Hessian estimation recursion, again with the objective of improving the quality of the Hessian estimate.
\end{inparaenum}

\tikzstyle{block} = [draw, fill=white, rectangle,
   minimum height=3em, minimum width=6em]
\tikzstyle{sum} = [draw, fill=white, circle, node distance=1cm]
\tikzstyle{input} = [coordinate]
\tikzstyle{output} = [coordinate]
\tikzstyle{pinstyle} = [pin edge={to-,thin,black}]

 \begin{figure}[t]
    \centering
\scalebox{0.8}{\begin{tikzpicture}[auto, node distance=2cm,>=latex']
% We start by placing the blocks
\node (theta) {$\boldsymbol{x_n}$};
\node [block, fill=blue!20,right=0.6cm of theta,align=center] (sample) {\makecell{\textbf{Measurement}\\\textbf{ Oracle}}}; 
\node [right=0.6cm of sample] (end) {$\boldsymbol{\mathbf{f(x_n) + \xi_n}}$};
\node [ above right= 0.6cm of end] (bias) {\textbf{Zero mean}};
\draw [->] (theta) --  (sample);
\draw [->] (sample) -- (end);
\path [darkgreen,->] (bias) edge [bend left] (end);
\end{tikzpicture}}
\caption{Simulation optimization}
\label{fig:so}
\end{figure}

Items (i) and (ii) are inspired by the corresponding improvements to the Hessian estimation recursion in 2SPSA - see \cite{spall-jacobian}. While item (ii) is a relatively straightforward migration to 2RDSA setting, item (i) is a non-trivial contribution, primarily because the Hessian estimate in 2RDSA is entirely different from that in 2SPSA and the feedback term that we incorporate in 2RDSA to improve the Hessian estimate neither correlates to that in 2SPSA nor follows from the analysis in \cite{spall-jacobian}. Further, we show empirically that 2RDSA with feedback that we propose here outperforms both 2SPSA with feedback and 2RDSA without feedback. Our contribution is important because 2RDSA with feedback, like 2RDSA, has lower simulation cost per iteration than 2SPSA and unlike 2RDSA, has an improved Hessian estimation scheme.


\section{SECOND-ORDER RDSA WITH IMPROVED HESSIAN ESTIMATION (2RDSA-IH)}
The second-order RDSA with improved Hessian estimate performs an update iteration as follows:
\begin{align}
\label{eq:e2rdsa}
x_{n+1} = x_n - a_n \Upsilon(\overline H_n)^{-1}\widehat\nabla f(x_n), \\
\overline H_n = (1-w_{n})  \overline H_{n-1} + w_{n} ( \widehat H_n - \widehat \Psi_n),\label{eq:2rdsa-H}
\end{align}
where $\widehat\nabla f(x_n)$ is the estimate of $\nabla f(x_n)$, 
 %and this corresponds to  the uniform  and   asymmetric Bernoulli variant gradient estimates proposed in \cite{prashanth2015rdsa}.
$\overline H_n$ is an estimate of the true Hessian ${\nabla}^2 f(\cdot)$, $\Upsilon(\cdot)$ projects any matrix onto the set of positive definite matrices and $\{a_n, n\ge 0\}$ is a step-size sequence that satisfies standard stochastic approximation conditions.  

The recursion \eqref{eq:e2rdsa} is identical to that in 2RDSA, while the Hessian estimation recursion \eqref{eq:2rdsa-H} differs as follows:\\
\begin{inparaenum}[\bfseries (i)]
\item  $\widehat \Psi_n$ is a zero-mean feedback term that reduces the error in Hessian estimate; and\\
\item $w_n$ is a general step-size that we optimize to improve the Hessian estimate.
\end{inparaenum}

On the other hand, $\widehat H_n$ is identical to that in 2RDSA, i.e., it estimates the true Hessian in each iteration using $3$ function evaluations. %Further, $\widehat H_0 = I$. 
For the sake of completeness, we provide below the construction for $\widehat\nabla f(x_n)$ and $\widehat H_n$ using asymmetric Bernoulli perturbations, before we present the feedback term that reduces the error in $\widehat H_n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\algblock{PEval}{EndPEval}
\algnewcommand\algorithmicPEval{\textbf{\em Function evaluation (Trajectory 1)}}
 \algnewcommand\algorithmicendPEval{}
\algrenewtext{PEval}[1]{\algorithmicPEval\ #1}
\algrenewtext{EndPEval}{\algorithmicendPEval}

\algblock{PEvalPrime}{EndPEvalPrime}
\algnewcommand\algorithmicPEvalPrime{\textbf{\em Function evaluation (Trajectory 2)}}
 \algnewcommand\algorithmicendPEvalPrime{}
\algrenewtext{PEvalPrime}[1]{\algorithmicPEvalPrime\ #1}
\algrenewtext{EndPEvalPrime}{\algorithmicendPEvalPrime}

\algblock{PImp}{EndPImp}
\algnewcommand\algorithmicPImp{\textbf{\em Gradient descent}}
 \algnewcommand\algorithmicendPImp{}
\algrenewtext{PImp}[1]{\algorithmicPImp\ #1}
\algrenewtext{EndPImp}{\algorithmicendPImp}

\algtext*{EndPEval}
\algtext*{EndPEvalPrime}
\algtext*{EndPImp}
%%%%%%%%%%%%%%%% alg-custom-block %%%%%%%%%%%%
\algblock{PEvalPrimeDouble}{EndPEvalPrimeDouble}
\algnewcommand\algorithmicPEvalPrimeDouble{\textbf{\em Function evaluation (Trajectory 3)}}
 \algnewcommand\algorithmicendPEvalPrimeDouble{}
\algrenewtext{PEvalPrimeDouble}[1]{\algorithmicPEvalPrimeDouble\ #1}
\algrenewtext{EndPEvalPrimeDouble}{\algorithmicendPEvalPrimeDouble}
\algtext*{EndPEvalPrimeDouble}

\algblock{PImpNewton}{EndPImpNewton}
\algnewcommand\algorithmicPImpNewton{\textbf{\em Newton step}}
 \algnewcommand\algorithmicendPImpNewton{}
\algrenewtext{PImpNewton}[1]{\algorithmicPImpNewton\ #1}
\algrenewtext{EndPImpNewton}{\algorithmicendPImpNewton}

\algtext*{EndPImpNewton}

%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[t]
\begin{algorithmic}
\State {\bf Input:} 
initial parameter $x_0 \in \R^N$, perturbation constants $\delta_n>0$, step-sizes $\{a_n, w_n\}$, operator $\Upsilon$.
\For{$n = 0,1,2,\ldots$}	
	\State Generate $\{d_n^{i}, i=1,\ldots,N\}$ using asymmetric Bernoulli distribution \eqref{eq:det-proj}, independent of $\{d_m, m=0,1,\ldots,n-1\}$.
	\PEval
	    \State Obtain $f(x_n+\delta_n d_n)$.
  \EndPEval
	    \PEvalPrime
	    \State Obtain $f(x_n-\delta_n d_n)$.
	    \EndPEvalPrime
	    	    \PEvalPrimeDouble
	    \State Obtain $f(x_n)$.
	    \EndPEvalPrimeDouble
	    \PImpNewton
		\State Update the parameter and Hessian according to \eqref{eq:e2rdsa}--\eqref{eq:2rdsa-H}.
		\EndPImpNewton
\EndFor
\State {\bf Return} $x_n.$
\end{algorithmic}
\caption{Structure of 2RDSA-AsymBer-IH algorithm.}
\label{alg:structure-2}
\end{algorithm}


\paragraph{Function evaluations}
Let $y_n$, $y_n^+$ and $y_n^-$ denote the function evaluations at $x_n$, $x_n+\delta_n d_n$ and $x_n - \delta_n d_n$ respectively, i.e., 
$y_n = f(x_n) + \xi_n$, $y_n^+ = f(x_n+\delta_n d_n) + \xi_n^+$ and 
$y_n^- = f(x_n-\delta_n d_n) + \xi_n^-$,
where the noise terms $\xi_n, \xi_n^+, \xi_n^-$ satisfy $\E\left[\left.\xi_n^+ + \xi_n^- - 2\xi_n \right| \F_n\right] = 0$ with $\F_n = \sigma(x_m,m\le n)$ denoting the underlying sigma-field.

\paragraph{Gradient estimate}
The RDSA estimate of the gradient $\nabla f(x_n)$ is given by
\begin{align}
\label{eq:grad-ber}
\widehat\nabla f(x_n) = \frac1{1+\epsilon} d_n \left[ \dfrac{y_n^+ - y_n^-}{2\delta_n}\right],
\end{align}
where the perturbations $d_n^i$, $i=1,\ldots,N$ are i.i.d. and distributed  as follows: 
\begin{equation}
\label{eq:det-proj}
 d_n^i =
  \begin{cases}
   -1 &  \text{ w.p. } \dfrac{(1+\epsilon)}{(2+\epsilon)}, \\
   1+\epsilon &  \text{ w.p. } \dfrac{1}{(2+\epsilon)},
  \end{cases}
\end{equation}
where $\epsilon>0$ is a constant that can be chosen to be arbitrarily small.
%Note that $E d_n^i = 0$, $E (d_n^i)^2 = 1+\epsilon$ and $E (d_n^i)^4 = \dfrac{(1+\epsilon)(1+(1+\epsilon)^3)}{(2+\epsilon)}$.

\paragraph{Hessian estimate}

\begin{align}
\label{eq:2rdsa-estimate-ber}
&\widehat H_n = M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right), \text{ where }\\
& M_n =
\left[
\begin{array}{ccc}
\frac{1}{\kappa}\left((d_n^1)^2\!-(1+\epsilon)\right) & \cdots & \frac{1}{2(1+\epsilon)^2}d_n^1 d_n^N\\
\frac{1}{2(1+\epsilon)^2}d_n^2 d_n^1  &  \cdots & \frac{1}{2(1+\epsilon)^2}d_n^2 d_n^N\\
\frac{1}{2(1+\epsilon)^2}d_n^N d_n^1 & \cdots &  \frac{1}{\kappa}\left((d_n^N)^2-(1+\epsilon)\right) \\
\end{array}
\right],\nonumber
\end{align}
where $\kappa = \tau \left(1- \dfrac{(1+\epsilon)^2}{\tau}\right)$ and $\tau = E (d_n^i)^4= \dfrac{(1+\epsilon)(1+(1+\epsilon)^3)}{(2+\epsilon)}$, for any $i=1,\ldots,N$. 
%%%%%% Feedback
\paragraph{Feedback term $\widehat \Psi_n$}
The Hessian estimate $\widehat H_n$ can be simplified as follows: 
\begin{align}
\widehat H_n & = M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right) \nonumber \\
& =  M_n \left[\left(\dfrac{f(x_n+\delta_n d_n) + f(x_n-\delta_n d_n) - 2 f(x_n)}{\delta_n^2}\right) \right.\nonumber \\&\hspace{10em}+\left. \left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2}\right)\right] \nonumber \\
& = M_n \left(d_n\tr \nabla^2 f(x_n) d_n +  O(\delta_n^2) + \left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2} \right)\right)\label{eq:h0}
%& = M_n \left(d_n\tr \nabla^2 f(x_n) d_n +  O(\delta_n^2) + O(\delta_n^{-2})\right). \label{eq:h0}
\end{align}
Analyzing the first term on the RHS above, we obtain
\begin{align}
&\E[M_n \left(d_n\tr \nabla^2 f(x_n) d_n\right) \mid \F_n] =   \E\left[\left. M_n \times\right.\right.\nonumber\\
&\left.\left.\left(\sum\limits_{i=1}^{N-1} (d_n^i)^2 \nabla^2_{ii} f(x_n) + 2\sum\limits_{i=1}^N\sum\limits_{j=i+1}^N d_n^i d_n^j \nabla^2_{ij} f(x_n) \right)\right| \F_n\right]. \label{eq:h1}
\end{align}
In analyzing the $l$th diagonal term of above expression, the following zero-mean term appears (see the proof of Lemma 4 in \cite{prashanth2015rdsa}):
\begin{align}
& \E\left[\left. \left([M_n]_{D}\right)_{l,l} \left(2\sum\limits_{i=1}^{N-1}\sum\limits_{j=i+1}^N d_n^i d_n^j \nabla^2_{ij} f(x_n)\right)\right| \F_{n}\right] = 0,\label{eq:diagzero}
\end{align}
where for any matrix $M$, $[M]_{D}$ refers to a matrix that retains only the diagonal entries of $M$ and zeroes out the rest of the entries and $\left([M]_{D}\right)_{i,j}$ refers to the $(i,j)$th entry in $[M]_D$. We shall also use $[M]_{N}$ to refer to a matrix that retains only the off-diagonal entries of $M$, while zeroing out the diagonal ones.

The term on the LHS in \eqref{eq:diagzero}, denoted by $\Psi_{n}^{1}(\nabla^2 f(x_n))$, can be written in matrix form as follows: 
\begin{align}\label{eq:digmat}
\Psi_{n}^{1}(\nabla^2 f(x_n)) = [M_n]_{D}\left(d_{n}\tr \, [\nabla^2 f(x_n)]_{N} \, d_{n}\right).
\end{align}

In analyzing the off-diagonal term($(k,l)$ where $k < l$) of \eqref{eq:h1}, the following zero-mean term appears:
\begin{align}
& \E\left[\left.\left([M_n]_{D}\right)_{k,l}   \left(\sum\limits_{i=1}^N (d_n^i)^2 \nabla^2_{ii} f(x_n)\right)\right| \F_n \right] = 0. 
\end{align}
The term on the LHS above, denoted by $\Psi_{n}^{2}(\nabla^2 f(x_n))$, can be written in matrix form as follows: 
\begin{align}\label{eq:ndmat}
\Psi_{n}^{2}(\nabla^2 f(x_n)) = [M_n]_{N}\left(d_{n}\tr \, [\nabla^2 f(x_n)]_{D} \, d_{n}\right).
\end{align}
From the foregoing, the per-iteration Hessian estimate $\widehat H_n$ can be re-written as follows:
\begin{align}
 \E\left[\left.\widehat H_n\right| \F_n\right] = & \nabla^2 f(x_n) + \E\left[\left.\Psi_{n}(\nabla^2 f(x_n))\right| \F_n\right]  +  O(\delta_n^2)\nonumber\\
&+\E\left[\left.\left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2} \right)\right| \F_n\right], \label{eq:hnhat}
\end{align}
where, for any matrix $H$, 
\begin{align}
&\Psi_{n}(H) = \Psi_{n}^{1}(H) + \Psi_{n}^{2}(H)\nonumber\\
&\qquad= [M_n]_{D}\left(d_{n}\tr \, [H]_{N} \, d_{n}\right) +  [M_n]_{N}\left(d_{n}\tr \, [H]_{D} \, d_{n}\right).\label{eq:psi}
\end{align}
In the RHS of \eqref{eq:hnhat}, it is easy to see that that the second term involving $\Psi_{n}$ and the last term involving the noise are zero-mean. Moreover, since the noise is bounded by assumption, the last term in \eqref{eq:hnhat} vanishes asymptotically at the rate $O(\delta_n^{-2})$. So, the error in estimating the Hessian is due to the second term, which involves the perturbations $d_n$. This motivates the term $\widehat \Psi_n$ in the update rule \eqref{eq:e2rdsa}. 
 %and using the fact that $\E\left[\left.\xi_n^+ + \xi_n^- - 2\xi_n \right| \F_n\right] = 0$


Given that we operate in a simulation optimization setting, which implies $\nabla^2 f$ is not known, we construct the feedback term $\widehat \Psi_n$ in \eqref{eq:e2rdsa} by using $\overline H_{n-1}$ as a proxy for $\nabla^2 f$, i.e.,
\begin{align}
\widehat \Psi_n = \Psi_{n} (\overline H_{n-1}).
\end{align}

\paragraph{Optimizing step-sizes $w_n$}
Unlike the feedback term, adapting the idea of optimizing the step-sizes for 2RDSA is relatively straightforward from the corresponding approach for 2SPSA in \cite{spall-jacobian}. The difference here is that there exist only one $N$-dimensional perturbation vector $d_n$ in our setting, while 2SPSA had two $N$-dimensional vectors. This in turn implies that only the perturbation constant $\delta_n$ is needed in optimizing $w_n$.

The optimal choice for $w_n$ in \eqref{eq:2rdsa-H} is the following:
\begin{align}
\label{eq:wieghts}
w_i  = \delta_i^{4}/\sum\limits_{j=0}^{i} \delta_j^{4}.
\end{align}
The main idea behind above choice is provided below.
From \eqref{eq:hnhat}, we can infer that
\begin{align*}
\E \l \widehat H_n \r^2 \le \dfrac{C}{\delta_n^4} \text{ for some } C<\infty. 
\end{align*} 
This is because the third term in \eqref{eq:hnhat} vanishes asymptotically, while the fourth term there dominates asymptotically. Moreover, the noise factors in the fourth term in \eqref{eq:hnhat} are bounded above due to (C9) and independent of $n$, leaving the $\delta_n^2$ term in the denominator there. 

So, the optimization problem to be solved at instant $n$ is as follows:
\begin{align}
\sum \limits_{i=0}^{n} (\tilde w_k)^2 \delta_i^{-4}, \text{ subject to} \label{eq:wn-opt}\\
\tilde w_i \geq 0 ~\forall i \text{ and }\sum \limits_{i=0}^{n} \tilde w_i = 1.
\end{align}
In the above, $\tilde w_i$ are equivalent to the step-sizes $w_i$ in the following sense:
\begin{align}
\label{eq:hess}
\overline H_n = \sum\limits_{i=0}^{n} w_k^{(n)}(\widehat H_i -\widehat \Psi_i).
\end{align}
The solution to \eqref{eq:wn-opt} is achieved for $\tilde w_i^* = \delta_i^{4}/\sum \limits_{j=0}^{n} \delta_j^{4}, i=1,\ldots,n$. The optimal choice $\tilde w_i^*$ can be translated onto the step-sizes $w_i$, leading to \eqref{eq:wieghts}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{remark}\textbf{\textit{(Uniform perturbations)}}
In \cite{prashanth2015rdsa}, the authors suggest two alternatives for the distribution of random perturbations $d_n$: the asymmetric Bernoulli, which we described earlier and uniform that we outline next.

Choose $d_n^i$, $ i=1,\ldots,N$ to be i.i.d. $U[-\eta,\eta]$ for some $\eta>0$, where $U[-\eta,\eta]$ denotes the uniform distribution on the interval $[-\eta,\eta]$.
Then, the RDSA estimate of the gradient is given by
\begin{align}
\label{eq:grad-unif}
\widehat\nabla f(x_n) = \frac3{\eta^2} d_n \left[ \dfrac{y_n^+ - y_n^-}{2\delta_n}\right].
\end{align}

The Hessian estimate is given by
\begin{align}
\label{eq:2rdsa-estimate-unif}
&\widehat H_n =  M_n \left(\dfrac{y_n^+ + y_n^- - 2 y_n}{\delta_n^2}\right), \text{ where } \\
& M_n =
\dfrac{9}{2\eta^4}\left[
\begin{array}{cccc}
\frac{5}{2}\left((d_n^1)^2-\frac{\eta^2}{3}\right) & \cdots & d_n^1 d_n^N\\
d_n^2 d_n^1  &  \cdots & d_n^2 d_n^N\\
d_n^N d_n^1 & \cdots &  \frac{5}{2}\left((d_n^N)^2-\frac{\eta^2}{3}\right) \\
\end{array}
\right].\nonumber
\end{align}

The feedback term that we propose can be easily extended to the case of uniform perturbation, by using the $M_n$ as defined above instead of that for the asymmetric Bernoulli case.
\end{remark}
%Henceforth, we shall refer to algorithm \eqref{eq:2rdsa}--\eqref{eq:2rdsa-H} with Hessian estimate \eqref{eq:2rdsa-estimate} as 2RDSA.


\subsection{Main results}
\label{sec:2rdsa-results}

\input{results}




\begin{align}
 \widehat H_n =    \Phi_n(\nabla^2 f(x_n)) &+\Psi_{n}(\nabla^2 f(x_n)) +  O(\delta_n^2)\nonumber\\
&+\left(\dfrac{\xi_n^+ + \xi_n^- - 2 \xi_n}{\delta_n^2} \right) \label{eq:hnhat-ext}
\end{align}
Where, for any matrix $H$, 
\begin{align}
&\Phi_{n}(H) = [M_n]_{N}\left(d_{n}\tr \, [H]_{D} \, d_{n}\right) +  [M_n]_{D}\left(d_{n}\tr \, [H]_{N} \, d_{n}\right).
\end{align}
And $\Psi_n(H)$ is defined in the equation \eqref{eq:psi}.
\begin{theorem}(\textbf{})
\label{thm:quad-bound}
Suppose $f$ is quadratic function and only noise-free measurements  of $f$ are used to form $\widehat H_n$ in \eqref{eq:2rdsa-estimate-ber}. Suppose $0 < w_0\leq1$ and $w_n = w/n^r$, $n=1,2,\ldots,k$, where $1/2 < r < 1$ and $0 < w \leq 1$. Suppose the  conditions $\ldots\ldots\ldots\ldots\ldots\ldots\ldots$ hold and $d_n$ are identically distributed for each $n$. Further, Suppose $\nabla^2 f(x^*) > 0$ and $\Upsilon$ in \eqref{eq:e2rdsa} is such that $\E(\parallel \Upsilon(\overline H_n) - \overline H_n\parallel^2) = o(e^{-2wn^{1-r}/(1-r)})$ and $\parallel \Upsilon(H) - H \parallel^2 / (1+\parallel H \parallel^2)$ is uniformly bounded with respect to the set of symmetric $H \in \mathbb{R}^{p \times p}$. Then $trace[\E (\Lambda_n \tr \Lambda_n)] = O(e^{-2wn^{1-r} / {1-r}})$. Where $\Lambda = \overline H_k - H^*$
\end{theorem}
\begin{proof}
The proof consists of three parts. Those are as folllows
\begin{enumerate}[]
  \item proof of MSE convergence of $\overline H_k$.
  \item derivation of convenient representation of $trace[\E (\Lambda_n \tr \Lambda_n)]$.
  \item derivation of main big-$O$ result.
\end{enumerate}
\emph{$Part(i)$ : MSE convergence of $\overline H_n$ :} This part follows from theorem 3 of \cite{spall-jacobian} .\\
\emph{$Part(ii)$ :Representation of $trace[\E (\Lambda_n \tr \Lambda_n)]$ :} Let $\Lambda_n' = \Upsilon(\overline H_k) - H^*$. From \eqref{eq:2rdsa-H} we can write
\begin{align}\label{eq:2rdsa-quadthm}
\overline H_n = \overline H_{n-1} - w_n (\overline H_{n-1} - \hat H_n + \hat \Psi_n).
\end{align}
 From the equations \eqref{eq:2rdsa-quadthm} and using the fact for the quadratic case $\hat H_n = \Phi_n(H^*) + \Psi_n(H^*)$ ,  we can write 
 \begin{align}\label{eq:lambdak}
 \Lambda_n &= \Lambda_{n-1} - w_n ( H_{n-1} - \hat H_n + \hat \Psi_n) \nonumber\\&= (1-w_n) \Lambda_{n-1} - w_n (H^* + \hat \Psi_n - \hat H_n ) \nonumber\\&= (1-w_n) \Lambda_{n-1} - w_n (H^*+\hat \Psi_n -\Phi_n(H^*) - \Psi_n(H^*)\nonumber\\&= (1-w_n) \Lambda_{n-1} - w_n \Psi_n(\Upsilon(\overline H_{n-1})) \nonumber\\& \hspace{2.6cm}+ w_n (\Phi_k(H^*) - H^*)\nonumber\\&= (1-w_n) \Lambda_{n-1} - w_n\Psi_n(\Lambda_{n-1}')+ w_n (\Phi_k(H^*) - H^*)
 \end{align}
 Where last equality followed from the form of $\Psi(.)$ in the equation \eqref{eq:psi}. Then
 \begin{align}\label{lambda-exp}
 \Lambda_n  = & \left[ \prod_{k=1}^n (1-w_k) \right]\Lambda_0 \nonumber\\ &- \sum_{k=1}^n \left[\prod_{j=k+1}^n (1-w_j)\right] w_k \Psi_k(\Lambda_{k-1}') \nonumber\\ &+ \sum_{k=1}^n \left[\prod_{j=k+1}^n (1-w_j)\right] w_k (\Phi_k(H^*) - H^*) \hspace{0.2cm} a.s.
 \end{align}
 (note : $\prod_{j=n+1}^n (1-w_j) = 1$ for all n).
 Let us  characterise $trace[\E(\Lambda_n\tr \Lambda_n)]$ using \eqref{lambda-exp}. From independence of $d_k$ along k, \eqref{lambda-exp} represent martingale difference sequence, leading to 
 \begin{align}\label{eq:exp-lambda2}
 & \E (\Lambda_n \tr \Lambda_n) =  \left[ \prod_{k=1}^n (1-w_k) \right]^2 \E (\Lambda_0 \tr \Lambda_0)  \nonumber\\ &+ \sum_{k=1}^n \left[\prod_{j=k+1}^n (1-w_j)\right]^2 w_k^2   \E (\Psi_k(\Lambda_{k-1}') \tr \Psi_k(\Lambda_{k-1}'))  \nonumber\\ &+ \sum_{k=1}^n \left[\prod_{j=k+1}^n (1-w_j)\right]^2 w_k^2  \nonumber\\& \hspace{2cm} \times \E ((\Phi_k(H^*) - H^*) \tr  (\Phi_k(H^*) - H^*)) \nonumber\\&+\sum_{k=1}^n \left[\prod_{j=k+1}^n (1-w_j)\right]^2 w_k^2 \E\left(\Psi_k \tr (\Phi_k(H^*) - H^*))\right).
 \end{align}
 From the independence of $d_k$'s and $\Lambda_{k-1}'$ and using Cauchy-Schwartz inequality we get 
 \begin{align}\label{eq:trace}
 & trace \left[\E (\Lambda_n \tr \Lambda_n)\right] \leq \left[ \prod_{k=1}^n (1-w_k) \right]^2 trace \left[\E (\Lambda_0 \tr \Lambda_0)\right]  \nonumber\\ &+ \sum_{k=1}^n \left[\prod_{j=k+1}^n (1-w_j)\right]^2 w_k^2  \,\,\tau\left(\E (\Lambda_{k-1}' \otimes \Lambda_{k-1}')\right)  \nonumber\\ &+ \sum_{k=1}^n \left[\prod_{j=k+1}^n (1-w_j)\right]^2 w_k^2  \nonumber\\&  \hspace{0.5cm}\times trace\left[\E ((\Phi_k(H^*) - H^*) \tr  (\Phi_k(H^*) - H^*))\right] + \nonumber\\ &+ \sum_{k=1}^n \left[\prod_{j=k+1}^n (1-w_j)\right]^2 w_k^2   \tau\left(\E (\Lambda_{k-1}' \otimes \Lambda_{k-1}')\right)^{1/2} \nonumber\\&  \times trace\left[\E ((\Phi_k(H^*) - H^*) \tr  (\Phi_k(H^*) - H^*))\right]^{1/2}.
 \end{align}
Note that $1=w_k = e^{-w_k}(1-O(w_k^2))$, where the $O(w_k^2)$ term is strictly positive for $0 < w_k <1$ by the convexity of $e^{-w_k}$. Let $w(i,j) = \sum_{k=i}^j w_k$ and $a_{kn} = \left[\prod_{i=k+1}^n (1- O(w_i^2))\right]^2$ and $a_{nn} = 1$. Then
\begin{align}\label{eq:wij}
 & trace \left[\E (\Lambda_n \tr \Lambda_n)\right] \leq e^{-2 w(1,n)} a_{0n} trace \left[\E (\Lambda_0 \tr \Lambda_0)\right]  \nonumber\\ &+ e^{-2 w(1,n)} \sum_{k=1}^n e^{2 w(1,k)} a_{kn} w_k^2  \,\,\tau\left(\E (\Lambda_{k-1}' \otimes \Lambda_{k-1}')\right)  \nonumber\\ &+ e^{-2 w(1,n)} \sum_{k=1}^n e^{2 w(1,k)} a_{kn} w_k^2  \nonumber\\&  \hspace{0.5cm}\times trace\left[\E ((\Phi_k(H^*) - H^*) \tr  (\Phi_k(H^*) - H^*))\right] \nonumber\\ &+ e^{-2 w(1,n)} \sum_{k=1}^n e^{2 w(1,k)} a_{kn} w_k^2  \tau\left(\E (\Lambda_{k-1}' \otimes \Lambda_{k-1}')\right)^{1/2} \nonumber\\&  \times trace\left[\E ((\Phi_k(H^*) - H^*) \tr  (\Phi_k(H^*) - H^*))\right]^{1/2}.
 \end{align} 
From the hypothesis $0 < w_k < 1$ for all $k \geq 2$ and $r > 0.5$, the $a_{kn}$ are uniformly bounded in magnitude. Further by the facts $\sum_{k=i}^j w_k \to \infty$ as $j-i \to \infty$.
\begin{align}\label{eq:intwts}
w(i,j) = \int_i^j \frac{w}{x^r} dx + O(1) &= \left(\frac{w}{1-r}\right)(j^{1-r}-i{1-r})\nonumber\\ &\hspace{2cm}+O(1)
\end{align}
From the \eqref{eq:wij} and \eqref{eq:intwts}
\begin{align}\label{eq:thmwts}
 & trace \left[\E (\Lambda_n \tr \Lambda_n)\right] =  e^{-2 w(1,n)} a_{0n} trace \left[\E (\Lambda_0 \tr \Lambda_0)\right]  \nonumber\\ &+ \bar a_n e^{-2 w n^{1-r}/(1-r)} \sum_{k=1}^n e^{2 w k^{1-r}/(1-r)}  \times \frac{w^2}{k^{2 r}} \nonumber\\& \hspace{3.5cm} \times \tau\left(\E (\Lambda_{k-1}' \otimes \Lambda_{k-1}')\right)  \nonumber\\ &+ \bar a_n e^{-2 w n^{1-r}/(1-r)} \sum_{k=1}^n e^{2 w k^{1-r}/(1-r)}  \times \frac{w^2}{k^{2 r}}  \nonumber\\&  \hspace{0.5cm}\times trace\left[\E ((\Phi_k(H^*) - H^*) \tr  (\Phi_k(H^*) - H^*))\right] \nonumber\\ &+ \bar a_n e^{-2 w n^{1-r}/(1-r)} \sum_{k=1}^n e^{2 w k^{1-r}/(1-r)}  \times \frac{w^2}{k^{2 r}}  \nonumber\\&\hspace{3.5cm}\times \tau\left(\E (\Lambda_{k-1}' \otimes \Lambda_{k-1}')\right)^{1/2}\nonumber\\&\times trace\left[\E ((\Phi_k(H^*) - H^*) \tr  (\Phi_k(H^*) - H^*))\right]^{1/2}.
\end{align} $\bar a_n$ is uniformly bounded in magnitude by the corresponding uniform boundedness of the $a_{kn}$.\\
\emph{$Part(iii)$ The big-O result on rate of convergence:}
 We have two extra terms term three and four for \eqref{eq:intwts} when compared to \cite{spall-jacobian}. Let us consider third term, we know that $trace\left[\E ((\Phi_k(H^*) - H^*) \tr  (\Phi_k(H^*) - H^*))\right]$ bounded uniformly for for every $k$. Then  $\bar b_n e^{-2 w n^{1-r}/(1-r)} \sum_{k=1}^n e^{2 w k^{1-r}/(1-r)}  \times \frac{w^2}{k^{2 r}} \to 0$ as $n \to \infty$ by Kronecker's lemma and $\sum_{k=1}^n \frac{1}{k^{2 r}} < \infty$ because $1/2 < r < 1$.  Again by using uniform boundedness of $trace\left[\E ((\Phi_k(H^*) - H^*) \tr  (\Phi_k(H^*) - H^*))\right]$ we can rewrite  the fourth term in \eqref{eq:intwts} as follows 
 \begin{align}\label{eq:fourthterm}
 \bar b_n e^{-2 w n^{1-r}/(1-r)} \sum_{k=1}^n e^{2 w k^{1-r}/(1-r)}  \frac{w^2}{k^{2 r}} \nonumber \\ \hspace{3cm}\times \tau\left(\E (\Lambda_{k-1}' \otimes \Lambda_{k-1}')\right)^{1/2}
 \end{align}
 Hence the contradiction proof given in theorem 3,part (iii) of \cite{spall-jacobian} will follow from now on by using the fact that term in \eqref{eq:fourthterm} is $O(second term)$ of \eqref{eq:thmwts}. 
 
 
\end{proof}
\section{SIMULATION EXPERIMENTS}
\label{sec:expts}
\subsection{Implementation}
We test the performance of 2RDSA-Unif, 2RDSA-AsymBer and 2SPSA, with/without improved Hessian estimation. 
2SPSA algorithm uses Bernoulli $\pm 1$-valued perturbations, while 2RDSA/2RDSA-IH come in two variants - one that uses $U[-1,1]$ distributed perturbations (referred to as 2RDSA-Unif/2RDSA-IH-Unif) and the other that uses asymmetric Bernoulli perturbations (referred to as 2RDSA-AsymBer/2RDSA-IH-AsymBer).

For the empirical evaluations, we use the following two loss functions in $N=10$ dimensions:
\paragraph{Quadratic loss}
\begin{align}
f(x) = x\tr A x + b\tr x,\label{eq:quadratic}
\end{align} 
The optimum $x^*$ for the above $f$ is such that each coordinate of $x^*$ is $-0.9091$, with $f(x^*) = -4.55$

\paragraph{Fourth-order loss}
\begin{align} 
f(x) = x\tr A\tr A x + 0.1 \sum_{j=1}^N (Ax)^3_j + 0.01 \sum_{j=1}^N (Ax)^4_j,\label{eq:4thorder}
 \end{align} 
The optimum $x^*$ for above $f$ is $x^*=0$, with $f(x^*) = 0$. 

In both functions, $A$ is such that $NA$ is an upper triangular matrix with each entry one, $b$ is the $N$-dimensional vector of ones and the noise structure is similar to that used in \cite{spall_adaptive}. For any $x$, the noise is $[x\tr, 1]z$, where $z \approx \N(0,\sigma^2 I_{11\times11})$. We perform experiments for noisy as well as noise-less settings, with $\sigma=0.1$ for the noisy case. 



For all algorithms, we set $\delta_n = 3.8/n^{0.101}$ and $a_n = 1/n^{0.6}$. These choices have been used  for 2SPSA implementations before (see \cite{spall_adaptive}) and have demonstrated good finite-sample performance empirically, while satisfying the theoretical requirements needed for asymptotic convergence.  For all the algorithms, the initial point $x_0$ is the $N$-dimensional vector of ones.  For both 2SPSA and 2RDSA/2RDSA-IH, an initial $20\%$ of the the simulation budget was used up by 1SPSA/1RDSA and the resulting iterate was used to initialize 2SPSA/2RDSA. The distribution parameter $\epsilon$ is set to $0.0001$ for 2RDSA and to $0.01$ for 1RDSA. 

\subsection{Results}
We use normalized loss and normalized MSE (NMSE) as performance metrics for evaluating the algorithms. 
NMSE is the ratio $\l x_{n_\text{end}} - x^* \r^2 / \l x_0 - x^*\r^2$, while normalized loss is the ratio $f(x_{n_\text{end}})/f(x_0)$.  Here $n_\text{end}$ denotes the iteration number when the algorithm stopped updating its parameter. Note that $n_\text{end}$ is a function of the simulation budget. 2RDSA/2RDSA-IH use only three simulations per-iteration and hence, $n_\text{end}$ is $1/3$rd of the simulation budget, while it is $1/4$th of the simulation budget for 2SPSA, since the latter algorithm uses four simulations per-iteration. 


\begin{table}
\centering
 \caption{Normalized loss values for fourth-order  objective \eqref{eq:4thorder} with and without noise: standard error from $500$ replications shown after $\pm$}
\label{tab:norloss-4thf}
\begin{tabular}{|c|c|c|}
\toprule
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0.1$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $0.0132 \pm 0.0267$ & $0.0104 \pm 0.0355$\\
&&\\
\textbf{2RDSA-Unif} &$0.0115 \pm 0.0214$ & $0.0271 \pm 0.0538$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $0.0471 \pm 0.021$& $\bm{0.0099 \pm 0.0014}$\\
 \bottomrule
 %%%%%%%%%%%%2nd ordr
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $0.0795 \pm 0.0234$ & $0.0628 \pm 0.0234$\\
&&\\
\textbf{2RDSA-Unif} &$0.0813 \pm 0.0275$ & $0.0214 \pm 0.00376$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $0.0199 \pm 0.0114$& $\bm{0.0098 \pm 0.00147}$\\
 \bottomrule
\end{tabular}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%% Quadratic
\begin{table}
\centering
 \caption{Normalized loss values for quadratic objective \eqref{eq:quadratic} with and without noise: standard error from $500$ replications shown after $\pm$}
\label{tab:norloss-quadratic}
\begin{tabular}{|c|c|c|}
\toprule
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0.1$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $-0.0062 \pm 0.1164$ & $-0.1229 \pm 0.1374$\\
&&\\
\textbf{2RDSA-Unif} &$0.0485 \pm 0.1465$ & $-0.259 \pm 0.0398$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $-0.2564 \pm 0.068$& $\bm{-0.2877 \pm 0.0051}$\\
 \bottomrule
 %%%%%%%%%%%%2nd ordr
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $-0.0785 \pm 0.1178$ & $-0.1716 \pm 0.1339$\\
&&\\
\textbf{2RDSA-Unif} &$0.0326 \pm 0.1599$ & $-0.2672 \pm 0.0299$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $-0.2777 \pm 0.0488$& $\bm{-0.2881 \pm 0.0012}$\\
 \bottomrule
\end{tabular}
\end{table}

%%%%%%%%%%%%%% NMSE for quadratic
\begin{table}
\centering
 \caption{NMSE values for quadratic objective \eqref{eq:quadratic} with and without noise: standard error from $500$ replications shown after $\pm$}
\label{tab:nmse-quadratic}
\begin{tabular}{|c|c|c|}
\toprule
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0.1$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $0.9491 \pm 0.0131$ & $0.5495 \pm 0.0217$\\
&&\\
\textbf{2RDSA-Unif} &$1.0073 \pm 0.0140$ & $0.1953 \pm 0.0095$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $0.1667 \pm 0.0095$& $\bm{0.0324 \pm 0.0007}$\\
 \bottomrule
 %%%%%%%%%%%%2nd ordr
\rowcolor{gray!20}
\multicolumn{3}{||c|}{\multirow{2}{*}{\textbf{Noise parameter $\sigma=0$}}}\\[1em]
%&&&\\
\midrule
  & \multirow{2}{*}{\textbf{Regular}} & \textbf{Improved Hessian}  \\
  & & \textbf{ estimation} \\
 \midrule
\textbf{2SPSA} & $0.7325 \pm 0.0180$ & $0.3939 \pm 0.0230$\\
&&\\
\textbf{2RDSA-Unif} &$0.9834 \pm 0.0170$ & $0.1623 \pm 0.0086$\\ 
&&\\
\textbf{2RDSA-AsymBer}& $0.0686 \pm 0.0078$& $\bm{0.0316 \pm 0.0006}$\\
 \bottomrule
\end{tabular}
\end{table}


\newcommand{\errorband}[5][]{ % x column, y column, error column, optional argument for setting style of the area plot
\pgfplotstableread[col sep=comma, skip first n=2]{#2}\datatable
    % Lower bound (invisible plot)
    \addplot [draw=none, stack plots=y, forget plot] table [
        x={#3},
        y expr=\thisrow{#4}-2*\thisrow{#5}
    ] {\datatable};

    % Stack twice the error, draw as area plot
    \addplot [draw=none, fill=gray!40, stack plots=y, area legend, #1] table [
        x={#3},
        y expr=4*\thisrow{#5}
    ] {\datatable} \closedcycle;

    % Reset stack using invisible plot
    \addplot [forget plot, stack plots=y,draw=none] table [x={#3}, y expr=-(\thisrow{#4}+2*\thisrow{#5})] {\datatable};
}

 \begin{figure}
    \centering
\tabl{c}{\scalebox{0.9}{\begin{tikzpicture}
      \begin{axis}[
	xlabel={number of simulations},
	ylabel={Normalized loss},
       legend entries={
	 ,2SPSA,,2SPSA-IH,,2RDSA-Unif,,2RDSA-Unif-IH,,2RDSA-AsymBer,,2RDSA-AsymBer-IH },
        %legend pos=outer north east,
			legend style={
at={(0,0)},
anchor=north,at={(axis description cs:0.5,-0.2)},legend columns=2}	]
      % 2SPSA
      \errorband[red!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{1}{2}
      \addplot [thick, red,mark=square*,mark options={scale=0.7}] table [x index=0, y index=1,col sep=comma] {norloss_vs_simulations_noisycase.csv};
			%%% With IH
      % 2SPSA
      \errorband[darkgray!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{7}{8}
      \addplot [thick, darkgray,mark=*,mark options={scale=0.75}] table [x index=0, y index=7,col sep=comma] {norloss_vs_simulations_noisycase.csv};
			
      % 2RDSA-Unif
      \errorband[blue!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{3}{4}
      \addplot [thick, blue,mark=square*,mark options={scale=0.7}] table [x index=0, y index=3,col sep=comma] {norloss_vs_simulations_noisycase.csv};
			%%% With IH
      % 2RDSA-Unif
      \errorband[magenta!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{9}{10}
      \addplot [thick, magenta,mark=*,mark options={scale=0.75}] table [x index=0, y index=9,col sep=comma] {norloss_vs_simulations_noisycase.csv};


      % 2RDSA-AsymBer
      \errorband[green!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{5}{6}
      \addplot [thick, darkgreen,mark=square*,mark options={scale=0.7}] table [x index=0, y index=5,col sep=comma] {norloss_vs_simulations_noisycase.csv};
			%%% With IH	
      % 2RDSA-AsymBer
      \errorband[violet!50!white, opacity=0.3]{norloss_vs_simulations_noisycase.csv}{0}{11}{12}
      \addplot [thick, violet, mark=*,mark options={scale=0.75}] table [x index=0, y index=11,col sep=comma] {norloss_vs_simulations_noisycase.csv};
      \end{axis}
      \end{tikzpicture}}\\}
			\caption{Normalized loss vs. number of simulations for fourth-order loss \eqref{eq:4thorder} with $\sigma=0.1$ for 2SPSA, 2RDSA-Unif and 2RDSA-AsymBer algorithms with/without improved Hessian estimation: bands around the curves represent standard error from $500$ replications. }
      \label{fig:norlossvssims} 
\end{figure} 


Tables \ref{tab:norloss-4thf}--\ref{tab:norloss-quadratic} present the normalized loss values observed for the three algorithms - 2SPSA, 2RDSA-Unif and 2RDSA-AsymBer - with/without feedback and for the fourth-order and quadratic loss functions, respectively. Table \ref{tab:nmse-quadratic} presents the NMSE values obtained for the aforementioned algorithms with the quadratic loss. Figure \ref{fig:norlossvssims} plots the normalized loss as a function of the simulation budget with the fourth-order loss objective with $\sigma=0.1$ (see Figure \ref{fig:norlossvssimsNoiseless} in the appendix similar results with $\sigma = 0$). From the results in Tables \ref{tab:norloss-4thf}--\ref{tab:nmse-quadratic} and Fig \ref{fig:norlossvssims}, we make the following observations:
 
\textit{\textbf{Observation 1:} Among 2RDSA schemes, 2RDSA-IH performs better than regular 2RDSA, for both perturbation choices.}

\textit{\textbf{Observation 2:} 2RDSA-IH variants outperform both 2SPSA and 2SPSA-IH, with 2RDSA-IH-AsymBer performing the best overall.}






\section{CONCLUSIONS}
to be done.

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}
\begin{figure}
    \centering
\tabl{c}{\scalebox{0.9}{\begin{tikzpicture}
      \begin{axis}[
	xlabel={number of simulations},
	ylabel={Normalized loss},
       legend entries={
	 ,2SPSA,,2SPSA-IH,,2RDSA-Unif,,2RDSA-Unif-IH,,2RDSA-AsymBer,,2RDSA-AsymBer-IH },
        %legend pos=outer north east,
			legend style={
at={(0,0)},
anchor=north,at={(axis description cs:0.5,-0.2)},legend columns=2}	]
      % 2SPSA
      \errorband[red!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{1}{2}
      \addplot [thick, red,mark=square*,mark options={scale=0.7}] table [x index=0, y index=1,col sep=comma] {norloss_vs_simulations.csv};
			%%% With IH
      % 2SPSA
      \errorband[darkgray!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{7}{8}
      \addplot [thick, darkgray,mark=*,mark options={scale=0.75}] table [x index=0, y index=7,col sep=comma] {norloss_vs_simulations.csv};
			
      % 2RDSA-Unif
      \errorband[blue!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{3}{4}
      \addplot [thick, blue,mark=square*,mark options={scale=0.7}] table [x index=0, y index=3,col sep=comma] {norloss_vs_simulations.csv};
			%%% With IH
      % 2RDSA-Unif
      \errorband[magenta!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{9}{10}
      \addplot [thick, magenta,mark=*,mark options={scale=0.75}] table [x index=0, y index=9,col sep=comma] {norloss_vs_simulations.csv};


      % 2RDSA-AsymBer
      \errorband[green!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{5}{6}
      \addplot [thick, darkgreen,mark=square*,mark options={scale=0.7}] table [x index=0, y index=5,col sep=comma] {norloss_vs_simulations.csv};
			%%% With IH	
      % 2RDSA-AsymBer
      \errorband[violet!50!white, opacity=0.3]{norloss_vs_simulations.csv}{0}{11}{12}
      \addplot [thick, violet, mark=*,mark options={scale=0.75}] table [x index=0, y index=11,col sep=comma] {norloss_vs_simulations.csv};
      \end{axis}
      \end{tikzpicture}}\\}
			\caption{Normalized loss vs. number of simulations for fourth-order loss \eqref{eq:4thorder} with $\sigma=0$ for 2SPSA, 2RDSA-Unif and 2RDSA-AsymBer algorithms with/without improved Hessian estimation: bands around the curves represent standard error from $500$ replications. }
      \label{fig:norlossvssimsNoiseless} 
\end{figure} 
 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\bibliographystyle{IEEEtran}
\bibliography{reference}



\end{document}
